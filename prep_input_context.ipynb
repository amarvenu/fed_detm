{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8fa91bd-8bba-4be7-b60d-a20505e719f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/amarvenu/miniconda3/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, BertTokenizer, T5Model, T5Tokenizer\n",
    "import torch\n",
    "import itertools\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e3f6db5-e3a1-4ef5-a734-d2348dc0fa02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/combined_clean.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b276ea7c-b599-41d0-bd23-bb44b5f763b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_vec = CountVectorizer(min_df=0.01, max_df=0.9).fit(df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f21e7b52-c1f7-4d7e-9aff-1823bc61dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save down the documents, filtered for words in the cv list\n",
    "def filter_text(text):\n",
    "    return ' '.join([x for x in text.split() if x in filter_vec.vocabulary_])\n",
    "\n",
    "df['filtered_text'] = df['clean_text'].apply(filter_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1226dcbd-ecb1-4f55-bf65-da50104a7cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4938"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer().fit(df['filtered_text'])\n",
    "counts = vectorizer.transform(df['clean_text'])\n",
    "counts.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30ed8330-329d-4750-b58b-53b426d3ae19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1650it [00:00, 4011.55it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokens = []\n",
    "train_counts = []\n",
    "for c in tqdm(counts):\n",
    "    train_tokens.append(np.nonzero(c)[1])\n",
    "    train_counts.append(np.squeeze(np.array(c[c > 0])))\n",
    "    \n",
    "train_tokens = np.array(train_tokens, dtype='object')\n",
    "train_counts = np.array(train_counts, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed009d10-7c6e-4813-82f8-35fc5827bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['date'] <= '2006-01-31', 'slice'] = 0  # Greenspan\n",
    "df.loc[('2006-02-01' <= df['date']) & (df['date'] <= '2014-01-31'), 'slice'] = 1  # Bernanke\n",
    "df.loc[('2014-02-03' <= df['date']) & (df['date'] < '2018-02-03'), 'slice'] = 2  # Yellen\n",
    "df.loc['2018-02-05' <= df['date'], 'slice'] = 3  # Powell\n",
    "\n",
    "df['slice'] = df['slice'].astype(int)\n",
    "\n",
    "train_times = df['slice'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e22e4e3-4791-4f2a-88f6-6c2af097ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('data/combined_clean.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24bd784d-0bdf-4434-8d76-d646c2c6b318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "slice\n",
       "0    716\n",
       "1    546\n",
       "2    201\n",
       "3    187\n",
       "Name: date, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('slice').date.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "552be838-dbf1-4eda-acab-b79b5bc4b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(vectorizer.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1e2c4-9663-42d1-870a-6b59423c6ff3",
   "metadata": {},
   "source": [
    "## prep embeddings\n",
    "\n",
    "### try to take avg token-level embedding across all of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd93596-150b-4030-934c-f66c488402bd",
   "metadata": {},
   "source": [
    "### bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7516158-df13-41cc-ab9f-90ede82219ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca8e1be7-7122-45ad-9b7a-199d3ef5711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len'] = df['filtered_text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d651d39-ca49-4880-a6dd-706572cb2a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4394"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = df.loc[df['len'] > 500].sort_values('len').iloc[:200]['filtered_text'].tolist()\n",
    "len(set(' '.join(sample).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cff26876-32c0-4efd-bc1d-3b37e3ec137e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2805"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## split them all to be no more than 400 words\n",
    "short_sample = []\n",
    "for x in sample:\n",
    "    if len(x) > 400:\n",
    "        for i in range(len(x) // 400 + 1):\n",
    "            short_sample.append(x[i*400:(i+1)*400])\n",
    "    else:\n",
    "        short_sample.append(x)\n",
    "len(short_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3254cb6-b0dd-43a4-82c6-fab26ac25330",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set(' '.join(short_sample).split())\n",
    "full_tokens = {}\n",
    "for word in unique_words:\n",
    "    full_tokens[word] = tokenizer.encode(word)[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "881c0453-ed06-484c-9d0b-80e3a0dbedc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5126"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tokens = set(itertools.chain.from_iterable(full_tokens.values()))\n",
    "len(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cc3f63d-1277-40eb-88a4-ab242711afca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2805/2805 [24:05<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "embs = dict(zip(unique_tokens, [torch.zeros(768)]*len(unique_tokens)))\n",
    "for text in tqdm(short_sample):\n",
    "    tokens = tokenizer.encode(text)  # get list of all tokens in the sentence    \n",
    "    emb = model(torch.tensor(tokens).unsqueeze(0)).last_hidden_state.squeeze(axis=0)\n",
    "    for i, tok in enumerate(tokens[1:-1]):\n",
    "        embs[tok] += emb[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f5ffbe9-1d25-4f3d-b6b8-181821a28565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2805/2805 [00:02<00:00, 1019.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# get the counts for each token\n",
    "token_counts = dict(zip(unique_tokens, [0]*len(unique_tokens)))\n",
    "for text in tqdm(short_sample):\n",
    "    tokens = tokenizer.encode(text)[1:-1]\n",
    "    for tok in tokens:\n",
    "        token_counts[tok] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb0a5ae0-4622-4223-9da2-d89c4d15d7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5126/5126 [00:00<00:00, 137377.57it/s]\n"
     ]
    }
   ],
   "source": [
    "avg_embs = {}\n",
    "for tok in tqdm(embs):\n",
    "    avg_embs[tok] = embs[tok] / token_counts[tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bb1cc3f-b383-46ec-b94e-cc68269ac9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 3037, 102]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('interest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d52bb31-daa4-44a4-b930-0500e72c56be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([avg_embs[4610], avg_embs[3037]]).mean(axis=0).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ab7930a-8470-42dd-916f-0c00a68b4385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6156/6156 [00:00<00:00, 41177.09it/s]\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = {}\n",
    "for word in tqdm(full_tokens):  # see all the words\n",
    "    tmp = []\n",
    "    for tok in full_tokens[word]:  # see all the tokens in the word\n",
    "        tmp.append(avg_embs[tok])\n",
    "    word_embeddings[word] = torch.stack(tmp).mean(axis=0).squeeze()  # take the avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eac90293-11f0-4c69-b086-aee3236ffb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4938/4938 [00:00<00:00, 160617.56it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.zeros((len(vectorizer.vocabulary_), 768))\n",
    "\n",
    "for w in tqdm(vectorizer.vocabulary_):\n",
    "    if w in word_embeddings:\n",
    "        idx = vectorizer.vocabulary_[w]\n",
    "        embeddings[idx] = word_embeddings[w].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b6b83b5-3c8c-49bd-9273-b04a8041ea6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(545, 768)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[embeddings.sum(axis=1) == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79239336-a140-4602-a31f-9958fb1046a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save down everything\n",
    "np.savez_compressed(\n",
    "    'test_data_bert_context.npz', \n",
    "    train_tokens=train_tokens,\n",
    "    train_counts=train_counts,\n",
    "    train_times=train_times,\n",
    "    vocab=np.array(vocab),\n",
    "    embeddings=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17d55b4-0130-4e3c-805f-234f1320cf39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
