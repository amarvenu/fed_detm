{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293077f9-46f1-4489-b8ac-b2de4bef194a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/amarvenu/miniconda3/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from src import data\n",
    "from src.detm import DETM\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from analysis_utils import get_detm_topics, topic_diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1309530f-89db-4946-b9ea-bce4cc36ea74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "680dc701-a07d-46a5-9cc4-d351bc25fef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### data and file related arguments\n",
    "arg_str = \"\"\"\n",
    "parser.add_argument('--dataset', type=str, default='un', help='name of corpus')\n",
    "parser.add_argument('--data_path', type=str, default='un/', help='directory containing data')\n",
    "parser.add_argument('--emb_path', type=str, default='skipgram/embeddings.txt', help='directory containing embeddings')\n",
    "parser.add_argument('--save_path', type=str, default='./results', help='path to save results')\n",
    "parser.add_argument('--batch_size', type=int, default=1000, help='number of documents in a batch for training')\n",
    "parser.add_argument('--min_df', type=int, default=100, help='to get the right data..minimum document frequency')\n",
    "\n",
    "### model-related arguments\n",
    "parser.add_argument('--num_topics', type=int, default=50, help='number of topics')\n",
    "parser.add_argument('--rho_size', type=int, default=300, help='dimension of rho')\n",
    "parser.add_argument('--emb_size', type=int, default=300, help='dimension of embeddings')\n",
    "parser.add_argument('--t_hidden_size', type=int, default=800, help='dimension of hidden space of q(theta)')\n",
    "parser.add_argument('--theta_act', type=str, default='relu', help='tanh, softplus, relu, rrelu, leakyrelu, elu, selu, glu)')\n",
    "parser.add_argument('--train_embeddings', type=int, default=1, help='whether to fix rho or train it')\n",
    "parser.add_argument('--eta_nlayers', type=int, default=3, help='number of layers for eta')\n",
    "parser.add_argument('--eta_hidden_size', type=int, default=200, help='number of hidden units for rnn')\n",
    "parser.add_argument('--delta', type=float, default=0.005, help='prior variance')\n",
    "\n",
    "### optimization-related arguments\n",
    "parser.add_argument('--lr', type=float, default=0.005, help='learning rate')\n",
    "parser.add_argument('--lr_factor', type=float, default=4.0, help='divide learning rate by this')\n",
    "parser.add_argument('--epochs', type=int, default=100, help='number of epochs to train')\n",
    "parser.add_argument('--mode', type=str, default='train', help='train or eval model')\n",
    "parser.add_argument('--optimizer', type=str, default='adam', help='choice of optimizer')\n",
    "parser.add_argument('--seed', type=int, default=2019, help='random seed (default: 1)')\n",
    "parser.add_argument('--enc_drop', type=float, default=0.0, help='dropout rate on encoder')\n",
    "parser.add_argument('--eta_dropout', type=float, default=0.0, help='dropout rate on rnn for eta')\n",
    "parser.add_argument('--clip', type=float, default=0.0, help='gradient clipping')\n",
    "parser.add_argument('--nonmono', type=int, default=10, help='number of bad hits allowed')\n",
    "parser.add_argument('--wdecay', type=float, default=1.2e-6, help='some l2 regularization')\n",
    "parser.add_argument('--anneal_lr', type=int, default=0, help='whether to anneal the learning rate or not')\n",
    "parser.add_argument('--bow_norm', type=int, default=1, help='normalize the bows or not')\n",
    "\n",
    "### evaluation, visualization, and logging-related arguments\n",
    "parser.add_argument('--num_words', type=int, default=20, help='number of words for topic viz')\n",
    "parser.add_argument('--log_interval', type=int, default=10, help='when to log training')\n",
    "parser.add_argument('--visualize_every', type=int, default=1, help='when to visualize results')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=1000, help='input batch size for evaluation')\n",
    "parser.add_argument('--load_from', type=str, default='', help='the name of the ckpt to eval from')\n",
    "parser.add_argument('--tc', type=int, default=0, help='whether to compute tc or not')\n",
    "\"\"\".split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c037c86-db04-4fa8-8fc5-13f99332813d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keys = [x.strip(\"parser.add_argument('\").split(',')[0].strip('--').strip(\"'\") for x in arg_str if (len(x) > 0) and (not x.startswith('#'))]\n",
    "values = [x.strip(\"parser.add_argument('\").split(',')[2].strip(\" default=\").strip(\"'\") for x in arg_str if (len(x) > 0) and (not x.startswith('#'))]\n",
    "tmp_dict = dict(zip(keys, values))\n",
    "\n",
    "for k, v in tmp_dict.items():\n",
    "    if v.isnumeric():\n",
    "        tmp_dict[k] = int(v)\n",
    "    elif ('.' in v) and (v[0].isnumeric()):\n",
    "        tmp_dict[k] = float(v)    \n",
    "\n",
    "args = AttrDict()\n",
    "args.update(tmp_dict)\n",
    "\n",
    "args.train_embeddings = 0\n",
    "args.rho_size = 768\n",
    "args.num_topics = 10\n",
    "args.batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "574d5721-f212-49cf-afe9-dddf97ee08d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_arr = np.load('test_data.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e20155d7-2fff-4d89-a127-ba89a66d01f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_tokens = train_arr['train_tokens']\n",
    "train_counts = train_arr['train_counts']\n",
    "train_times = train_arr['train_times']\n",
    "vocab = train_arr['vocab']\n",
    "embeddings = train_arr['embeddings']\n",
    "\n",
    "args.num_times = len(np.unique(train_times))\n",
    "args.num_docs_train = len(train_tokens)\n",
    "args.vocab_size = len(vocab)\n",
    "args.num_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a019bc9d-fbd7-4d37-a325-c9a25ce8356e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.num_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "21ada37c-a91d-4293-a147-7de92607ae8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0/2\n",
      "CPU times: user 3.83 s, sys: 471 ms, total: 4.3 s\n",
      "Wall time: 383 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_rnn_inp = data.get_rnn_input(train_tokens, train_counts, train_times, args.num_times, args.vocab_size, args.num_docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c6b0df7f-8ffb-4dea-84c5-3faa403ac5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(args['save_path']):\n",
    "    os.makedirs(args['save_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24c7d9fa-102e-4d7e-8734-53096816d0a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if args.mode == 'eval':\n",
    "    ckpt = args.load_from\n",
    "else:\n",
    "    ckpt = os.path.join(args.save_path, \n",
    "        'detm_{}_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_L_{}_minDF_{}_trainEmbeddings_{}'.format(\n",
    "        args.dataset, args.num_topics, args.t_hidden_size, args.optimizer, args.clip, args.theta_act, \n",
    "            args.lr, args.batch_size, args.rho_size, args.eta_nlayers, args.min_df, args.train_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "047e40c2-92aa-4707-a77c-3db9313831cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DETM(\n",
       "  (t_drop): Dropout(p=0.0, inplace=False)\n",
       "  (theta_act): ReLU()\n",
       "  (q_theta): Sequential(\n",
       "    (0): Linear(in_features=4948, out_features=800, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (mu_q_theta): Linear(in_features=800, out_features=10, bias=True)\n",
       "  (logsigma_q_theta): Linear(in_features=800, out_features=10, bias=True)\n",
       "  (q_eta_map): Linear(in_features=4938, out_features=200, bias=True)\n",
       "  (q_eta): LSTM(200, 200, num_layers=3)\n",
       "  (mu_q_eta): Linear(in_features=210, out_features=10, bias=True)\n",
       "  (logsigma_q_eta): Linear(in_features=210, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embeddings = torch.from_numpy(embeddings).to(device)\n",
    "args.embeddings_dim = embeddings.size()\n",
    "\n",
    "model = DETM(args, embeddings)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f54de551-16ca-419d-876e-1ba965d39d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if args.optimizer == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'adadelta':\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'asgd':\n",
    "    optimizer = optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\n",
    "else:\n",
    "    print('Defaulting to vanilla SGD')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "71a92f90-5e83-4bf4-a3a0-2d9390061275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    \"\"\"Train DETM on data for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    acc_loss = 0\n",
    "    acc_nll = 0\n",
    "    acc_kl_theta_loss = 0\n",
    "    acc_kl_eta_loss = 0\n",
    "    acc_kl_alpha_loss = 0\n",
    "    cnt = 0\n",
    "    indices = torch.randperm(args.num_docs_train)\n",
    "    indices = torch.split(indices, args.batch_size) \n",
    "    for idx, ind in enumerate(indices):\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        data_batch, times_batch = data.get_batch(\n",
    "            train_tokens, train_counts, ind, args.vocab_size, args.emb_size, temporal=True, times=train_times)\n",
    "        sums = data_batch.sum(1).unsqueeze(1)\n",
    "        if args.bow_norm:\n",
    "            normalized_data_batch = data_batch / sums\n",
    "        else:\n",
    "            normalized_data_batch = data_batch\n",
    "\n",
    "        loss, nll, kl_alpha, kl_eta, kl_theta = model(data_batch, normalized_data_batch, times_batch, train_rnn_inp, args.num_docs_train)\n",
    "        loss.backward()\n",
    "        if args.clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        acc_loss += torch.sum(loss).item()\n",
    "        acc_nll += torch.sum(nll).item()\n",
    "        acc_kl_theta_loss += torch.sum(kl_theta).item()\n",
    "        acc_kl_eta_loss += torch.sum(kl_eta).item()\n",
    "        acc_kl_alpha_loss += torch.sum(kl_alpha).item()\n",
    "        cnt += 1\n",
    "\n",
    "        if idx % args.log_interval == 0 and idx > 0:\n",
    "            cur_loss = round(acc_loss / cnt, 2) \n",
    "            cur_nll = round(acc_nll / cnt, 2) \n",
    "            cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "            cur_kl_eta = round(acc_kl_eta_loss / cnt, 2) \n",
    "            cur_kl_alpha = round(acc_kl_alpha_loss / cnt, 2) \n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            print('Epoch: {} .. batch: {}/{} .. LR: {} .. KL_theta: {} .. KL_eta: {} .. KL_alpha: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "                epoch, idx, len(indices), lr, cur_kl_theta, cur_kl_eta, cur_kl_alpha, cur_nll, cur_loss))\n",
    "    \n",
    "    cur_loss = round(acc_loss / cnt, 2) \n",
    "    cur_nll = round(acc_nll / cnt, 2) \n",
    "    cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "    cur_kl_eta = round(acc_kl_eta_loss / cnt, 2) \n",
    "    cur_kl_alpha = round(acc_kl_alpha_loss / cnt, 2) \n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    print('*'*100)\n",
    "    print('Epoch----->{} .. LR: {} .. KL_theta: {} .. KL_eta: {} .. KL_alpha: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "            epoch, lr, cur_kl_theta, cur_kl_eta, cur_kl_alpha, cur_nll, cur_loss))\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b51ea22c-39c7-4ece-a26c-2c97c817146f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 16055.03 .. KL_eta: 2699.23 .. KL_alpha: 11959263.09 .. Rec_loss: 19987842.91 .. NELBO: 31965860.55\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 18886.62 .. KL_eta: 2048.93 .. KL_alpha: 11799006.18 .. Rec_loss: 20247932.59 .. NELBO: 32067874.71\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 21484.79 .. KL_eta: 503.92 .. KL_alpha: 11008494.0 .. Rec_loss: 20054444.91 .. NELBO: 31084928.0\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 19715.92 .. KL_eta: 408.92 .. KL_alpha: 10860758.82 .. Rec_loss: 19759509.41 .. NELBO: 30640393.29\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 21407.02 .. KL_eta: 187.24 .. KL_alpha: 10199047.0 .. Rec_loss: 19774749.09 .. NELBO: 29995390.73\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 20768.94 .. KL_eta: 196.86 .. KL_alpha: 10049925.47 .. Rec_loss: 19442627.53 .. NELBO: 29513519.29\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 22123.59 .. KL_eta: 222.21 .. KL_alpha: 9406225.09 .. Rec_loss: 19605091.64 .. NELBO: 29033662.55\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 20013.88 .. KL_eta: 225.62 .. KL_alpha: 9297361.76 .. Rec_loss: 19383282.82 .. NELBO: 28700884.24\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 20586.22 .. KL_eta: 210.21 .. KL_alpha: 8722551.0 .. Rec_loss: 19169720.91 .. NELBO: 27913068.18\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 19726.67 .. KL_eta: 207.16 .. KL_alpha: 8602729.62 .. Rec_loss: 19154351.41 .. NELBO: 27777015.18\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 17444.23 .. KL_eta: 252.52 .. KL_alpha: 8091607.32 .. Rec_loss: 19027155.45 .. NELBO: 27136459.27\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 17257.29 .. KL_eta: 234.96 .. KL_alpha: 7988409.24 .. Rec_loss: 18967987.53 .. NELBO: 26973888.71\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 19114.77 .. KL_eta: 204.26 .. KL_alpha: 7504601.0 .. Rec_loss: 18787504.55 .. NELBO: 26311424.91\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 18816.42 .. KL_eta: 201.18 .. KL_alpha: 7408762.65 .. Rec_loss: 18922112.82 .. NELBO: 26349893.18\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 19421.43 .. KL_eta: 146.89 .. KL_alpha: 7038438.55 .. Rec_loss: 18983573.82 .. NELBO: 26041580.55\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 18646.25 .. KL_eta: 136.47 .. KL_alpha: 6938484.74 .. Rec_loss: 18644185.0 .. NELBO: 25601452.35\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 16829.86 .. KL_eta: 118.75 .. KL_alpha: 6553571.14 .. Rec_loss: 18488127.55 .. NELBO: 25058647.09\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 18097.14 .. KL_eta: 139.32 .. KL_alpha: 6471526.79 .. Rec_loss: 18583169.94 .. NELBO: 25072932.94\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 16405.26 .. KL_eta: 164.28 .. KL_alpha: 6143652.36 .. Rec_loss: 18621903.09 .. NELBO: 24782125.27\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 17215.61 .. KL_eta: 188.77 .. KL_alpha: 6092633.82 .. Rec_loss: 18535466.12 .. NELBO: 24645504.59\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 18176.04 .. KL_eta: 175.15 .. KL_alpha: 5778565.36 .. Rec_loss: 18376783.64 .. NELBO: 24173701.09\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 17376.05 .. KL_eta: 188.59 .. KL_alpha: 5706565.09 .. Rec_loss: 18439625.41 .. NELBO: 24163755.65\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 18182.78 .. KL_eta: 268.63 .. KL_alpha: 5421815.05 .. Rec_loss: 18445984.18 .. NELBO: 23886250.55\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 17869.37 .. KL_eta: 280.97 .. KL_alpha: 5363044.03 .. Rec_loss: 18303900.71 .. NELBO: 23685094.94\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 17156.34 .. KL_eta: 318.87 .. KL_alpha: 5114105.23 .. Rec_loss: 18467782.73 .. NELBO: 23599363.09\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 17514.49 .. KL_eta: 330.54 .. KL_alpha: 5059679.44 .. Rec_loss: 18219866.24 .. NELBO: 23297390.82\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 17841.58 .. KL_eta: 360.67 .. KL_alpha: 4819683.95 .. Rec_loss: 18069104.91 .. NELBO: 22906991.09\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 17335.13 .. KL_eta: 370.3 .. KL_alpha: 4784086.97 .. Rec_loss: 18110280.71 .. NELBO: 22912072.94\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 18328.91 .. KL_eta: 328.89 .. KL_alpha: 4558002.32 .. Rec_loss: 18095577.09 .. NELBO: 22672236.91\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 18015.07 .. KL_eta: 339.64 .. KL_alpha: 4512381.62 .. Rec_loss: 18001658.71 .. NELBO: 22532395.06\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 15508.37 .. KL_eta: 350.67 .. KL_alpha: 4323482.09 .. Rec_loss: 17990629.73 .. NELBO: 22329970.36\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 16307.48 .. KL_eta: 355.06 .. KL_alpha: 4294221.24 .. Rec_loss: 18038809.24 .. NELBO: 22349692.82\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 16772.71 .. KL_eta: 312.62 .. KL_alpha: 4114370.89 .. Rec_loss: 17443454.0 .. NELBO: 21574910.36\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 16940.75 .. KL_eta: 318.46 .. KL_alpha: 4080803.47 .. Rec_loss: 17961756.47 .. NELBO: 22059819.18\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 17498.61 .. KL_eta: 325.25 .. KL_alpha: 3918185.25 .. Rec_loss: 17948402.91 .. NELBO: 21884412.73\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 17178.6 .. KL_eta: 330.29 .. KL_alpha: 3880018.75 .. Rec_loss: 17928858.94 .. NELBO: 21826387.06\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 19468.64 .. KL_eta: 373.8 .. KL_alpha: 3721659.25 .. Rec_loss: 17464077.82 .. NELBO: 21205579.09\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 18786.05 .. KL_eta: 380.32 .. KL_alpha: 3700902.84 .. Rec_loss: 17807915.06 .. NELBO: 21527984.0\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 17837.46 .. KL_eta: 499.55 .. KL_alpha: 3557040.16 .. Rec_loss: 17755503.73 .. NELBO: 21330880.91\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 17575.68 .. KL_eta: 508.43 .. KL_alpha: 3529303.47 .. Rec_loss: 17769653.0 .. NELBO: 21317040.71\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 16908.6 .. KL_eta: 480.52 .. KL_alpha: 3395312.98 .. Rec_loss: 17769088.64 .. NELBO: 21181790.91\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 17534.74 .. KL_eta: 472.47 .. KL_alpha: 3359621.26 .. Rec_loss: 17736644.0 .. NELBO: 21114272.47\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 17112.26 .. KL_eta: 487.81 .. KL_alpha: 3241814.75 .. Rec_loss: 17859642.91 .. NELBO: 21119057.64\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 17164.9 .. KL_eta: 498.74 .. KL_alpha: 3213469.6 .. Rec_loss: 17732437.76 .. NELBO: 20963570.94\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 17036.71 .. KL_eta: 479.91 .. KL_alpha: 3092153.66 .. Rec_loss: 17833915.82 .. NELBO: 20943586.55\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 16593.48 .. KL_eta: 479.45 .. KL_alpha: 3067204.47 .. Rec_loss: 17661938.76 .. NELBO: 20746216.59\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 18190.38 .. KL_eta: 340.0 .. KL_alpha: 2964838.66 .. Rec_loss: 17659940.91 .. NELBO: 20643309.82\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 18807.59 .. KL_eta: 331.61 .. KL_alpha: 2947081.6 .. Rec_loss: 17653153.0 .. NELBO: 20619373.65\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 17866.61 .. KL_eta: 401.57 .. KL_alpha: 2863587.3 .. Rec_loss: 17617436.91 .. NELBO: 20499292.0\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 18088.51 .. KL_eta: 419.46 .. KL_alpha: 2833433.24 .. Rec_loss: 17620413.29 .. NELBO: 20472354.0\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 18231.18 .. KL_eta: 380.94 .. KL_alpha: 2741804.75 .. Rec_loss: 17605776.18 .. NELBO: 20366192.73\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 18343.39 .. KL_eta: 398.33 .. KL_alpha: 2729593.69 .. Rec_loss: 17642693.71 .. NELBO: 20391028.59\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 18729.84 .. KL_eta: 461.6 .. KL_alpha: 2632186.89 .. Rec_loss: 17814053.55 .. NELBO: 20465431.27\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 18327.97 .. KL_eta: 455.35 .. KL_alpha: 2618498.46 .. Rec_loss: 17523817.88 .. NELBO: 20161099.53\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 17831.92 .. KL_eta: 455.56 .. KL_alpha: 2544155.89 .. Rec_loss: 17575154.36 .. NELBO: 20137597.64\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 17934.79 .. KL_eta: 431.37 .. KL_alpha: 2529850.15 .. Rec_loss: 17582179.29 .. NELBO: 20130395.41\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 19970.56 .. KL_eta: 352.86 .. KL_alpha: 2446991.36 .. Rec_loss: 17415879.27 .. NELBO: 19883194.36\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 18946.49 .. KL_eta: 350.85 .. KL_alpha: 2432502.4 .. Rec_loss: 17518284.29 .. NELBO: 19970084.12\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 20114.27 .. KL_eta: 419.96 .. KL_alpha: 2357553.43 .. Rec_loss: 17412614.0 .. NELBO: 19790701.45\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 19002.52 .. KL_eta: 422.77 .. KL_alpha: 2343348.16 .. Rec_loss: 17480719.29 .. NELBO: 19843492.82\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 19631.67 .. KL_eta: 421.82 .. KL_alpha: 2273137.32 .. Rec_loss: 17245172.55 .. NELBO: 19538362.73\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 19386.18 .. KL_eta: 394.33 .. KL_alpha: 2260087.21 .. Rec_loss: 17493938.29 .. NELBO: 19773805.53\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 18307.22 .. KL_eta: 315.0 .. KL_alpha: 2188819.11 .. Rec_loss: 17517875.82 .. NELBO: 19725316.91\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 18954.35 .. KL_eta: 331.47 .. KL_alpha: 2184291.18 .. Rec_loss: 17379219.29 .. NELBO: 19582796.0\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 18090.07 .. KL_eta: 329.9 .. KL_alpha: 2121093.23 .. Rec_loss: 17360814.73 .. NELBO: 19500327.09\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 18405.47 .. KL_eta: 336.9 .. KL_alpha: 2115335.98 .. Rec_loss: 17413972.82 .. NELBO: 19548050.59\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 19653.3 .. KL_eta: 337.64 .. KL_alpha: 2058409.95 .. Rec_loss: 17192982.18 .. NELBO: 19271383.09\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 19118.5 .. KL_eta: 324.57 .. KL_alpha: 2046068.53 .. Rec_loss: 17411909.76 .. NELBO: 19477421.41\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 20015.82 .. KL_eta: 362.54 .. KL_alpha: 1984147.49 .. Rec_loss: 17224786.82 .. NELBO: 19229312.36\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 19063.73 .. KL_eta: 371.25 .. KL_alpha: 1975196.99 .. Rec_loss: 17342500.29 .. NELBO: 19337132.12\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 20170.57 .. KL_eta: 375.86 .. KL_alpha: 1934298.38 .. Rec_loss: 17283513.09 .. NELBO: 19238358.0\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 19976.44 .. KL_eta: 369.71 .. KL_alpha: 1922366.86 .. Rec_loss: 17339073.18 .. NELBO: 19281786.24\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 19390.28 .. KL_eta: 337.77 .. KL_alpha: 1868055.62 .. Rec_loss: 17633606.0 .. NELBO: 19521389.64\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 19507.33 .. KL_eta: 338.53 .. KL_alpha: 1856853.6 .. Rec_loss: 17363905.06 .. NELBO: 19240604.41\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 18725.22 .. KL_eta: 379.47 .. KL_alpha: 1814255.24 .. Rec_loss: 17454866.09 .. NELBO: 19288225.64\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 19603.32 .. KL_eta: 384.7 .. KL_alpha: 1802838.7 .. Rec_loss: 17285976.94 .. NELBO: 19108803.53\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 19863.96 .. KL_eta: 367.08 .. KL_alpha: 1755148.66 .. Rec_loss: 17187924.18 .. NELBO: 18963304.0\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 19864.9 .. KL_eta: 384.08 .. KL_alpha: 1746983.75 .. Rec_loss: 17306379.18 .. NELBO: 19073612.12\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 19765.56 .. KL_eta: 304.84 .. KL_alpha: 1711742.15 .. Rec_loss: 17127271.0 .. NELBO: 18859083.27\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 19643.17 .. KL_eta: 288.6 .. KL_alpha: 1701967.6 .. Rec_loss: 17322008.24 .. NELBO: 19043907.29\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 21171.63 .. KL_eta: 283.27 .. KL_alpha: 1662608.42 .. Rec_loss: 17355737.55 .. NELBO: 19039800.36\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 20646.14 .. KL_eta: 301.72 .. KL_alpha: 1649741.11 .. Rec_loss: 17191632.29 .. NELBO: 18862320.88\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 20401.4 .. KL_eta: 424.65 .. KL_alpha: 1612495.03 .. Rec_loss: 17068205.18 .. NELBO: 18701526.18\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 20363.23 .. KL_eta: 400.38 .. KL_alpha: 1604052.33 .. Rec_loss: 17289795.35 .. NELBO: 18914611.41\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 20276.69 .. KL_eta: 231.58 .. KL_alpha: 1561241.6 .. Rec_loss: 16732570.91 .. NELBO: 18314320.55\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 20582.17 .. KL_eta: 221.46 .. KL_alpha: 1551660.25 .. Rec_loss: 17282151.47 .. NELBO: 18854615.29\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 21307.54 .. KL_eta: 239.87 .. KL_alpha: 1527969.58 .. Rec_loss: 17318706.55 .. NELBO: 18868223.64\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 20861.42 .. KL_eta: 253.57 .. KL_alpha: 1521934.48 .. Rec_loss: 17203302.59 .. NELBO: 18746352.12\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 20176.36 .. KL_eta: 306.9 .. KL_alpha: 1482554.4 .. Rec_loss: 17084460.0 .. NELBO: 18587497.82\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 20672.27 .. KL_eta: 304.62 .. KL_alpha: 1475408.9 .. Rec_loss: 17257682.76 .. NELBO: 18754068.59\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 21691.13 .. KL_eta: 252.91 .. KL_alpha: 1443950.77 .. Rec_loss: 16890996.64 .. NELBO: 18356891.64\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 21147.26 .. KL_eta: 258.2 .. KL_alpha: 1435724.85 .. Rec_loss: 17189201.82 .. NELBO: 18646332.24\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 21054.5 .. KL_eta: 287.89 .. KL_alpha: 1402068.22 .. Rec_loss: 17029136.91 .. NELBO: 18452547.82\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 21107.06 .. KL_eta: 302.1 .. KL_alpha: 1396227.71 .. Rec_loss: 17171586.06 .. NELBO: 18589223.18\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 20521.58 .. KL_eta: 318.52 .. KL_alpha: 1371685.17 .. Rec_loss: 17161878.82 .. NELBO: 18554404.0\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 20346.47 .. KL_eta: 304.17 .. KL_alpha: 1365830.58 .. Rec_loss: 17162757.06 .. NELBO: 18549238.35\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 20931.53 .. KL_eta: 253.8 .. KL_alpha: 1332300.6 .. Rec_loss: 16955933.09 .. NELBO: 18309418.91\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 20819.16 .. KL_eta: 252.92 .. KL_alpha: 1328131.74 .. Rec_loss: 17135950.88 .. NELBO: 18485154.71\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 22043.99 .. KL_eta: 292.78 .. KL_alpha: 1304661.76 .. Rec_loss: 17102916.91 .. NELBO: 18429916.0\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 21401.89 .. KL_eta: 280.81 .. KL_alpha: 1296512.28 .. Rec_loss: 17158728.24 .. NELBO: 18476923.65\n",
      "****************************************************************************************************\n",
      "Epoch: 51 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 21075.69 .. KL_eta: 233.96 .. KL_alpha: 1263299.93 .. Rec_loss: 17490714.82 .. NELBO: 18775324.73\n",
      "****************************************************************************************************\n",
      "Epoch----->51 .. LR: 0.005 .. KL_theta: 21133.43 .. KL_eta: 228.85 .. KL_alpha: 1256842.66 .. Rec_loss: 17110222.82 .. NELBO: 18388427.88\n",
      "****************************************************************************************************\n",
      "Epoch: 52 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 21912.46 .. KL_eta: 201.68 .. KL_alpha: 1239735.33 .. Rec_loss: 16942765.27 .. NELBO: 18204614.55\n",
      "****************************************************************************************************\n",
      "Epoch----->52 .. LR: 0.005 .. KL_theta: 21467.28 .. KL_eta: 187.73 .. KL_alpha: 1232950.19 .. Rec_loss: 17130610.76 .. NELBO: 18385216.0\n",
      "****************************************************************************************************\n",
      "Epoch: 53 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 22122.04 .. KL_eta: 249.95 .. KL_alpha: 1208307.8 .. Rec_loss: 17044399.91 .. NELBO: 18275080.09\n",
      "****************************************************************************************************\n",
      "Epoch----->53 .. LR: 0.005 .. KL_theta: 21458.48 .. KL_eta: 250.16 .. KL_alpha: 1202514.96 .. Rec_loss: 17088066.82 .. NELBO: 18312290.65\n",
      "****************************************************************************************************\n",
      "Epoch: 54 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 22011.3 .. KL_eta: 291.83 .. KL_alpha: 1176357.51 .. Rec_loss: 17059575.27 .. NELBO: 18258236.18\n",
      "****************************************************************************************************\n",
      "Epoch----->54 .. LR: 0.005 .. KL_theta: 21526.27 .. KL_eta: 282.0 .. KL_alpha: 1169058.6 .. Rec_loss: 17134988.94 .. NELBO: 18325856.12\n",
      "****************************************************************************************************\n",
      "Epoch: 55 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 21590.76 .. KL_eta: 224.11 .. KL_alpha: 1144370.82 .. Rec_loss: 16809641.73 .. NELBO: 17975827.27\n",
      "****************************************************************************************************\n",
      "Epoch----->55 .. LR: 0.005 .. KL_theta: 22434.24 .. KL_eta: 230.29 .. KL_alpha: 1139680.23 .. Rec_loss: 17034129.24 .. NELBO: 18196473.76\n",
      "****************************************************************************************************\n",
      "Epoch: 56 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 22047.54 .. KL_eta: 213.92 .. KL_alpha: 1118724.52 .. Rec_loss: 17037242.64 .. NELBO: 18178228.73\n",
      "****************************************************************************************************\n",
      "Epoch----->56 .. LR: 0.005 .. KL_theta: 22033.65 .. KL_eta: 232.17 .. KL_alpha: 1111161.06 .. Rec_loss: 17091348.35 .. NELBO: 18224775.18\n",
      "****************************************************************************************************\n",
      "Epoch: 57 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 22295.43 .. KL_eta: 223.2 .. KL_alpha: 1095691.56 .. Rec_loss: 17252430.55 .. NELBO: 18370640.36\n",
      "****************************************************************************************************\n",
      "Epoch----->57 .. LR: 0.005 .. KL_theta: 22374.53 .. KL_eta: 216.02 .. KL_alpha: 1088832.39 .. Rec_loss: 17001562.53 .. NELBO: 18112985.12\n",
      "****************************************************************************************************\n",
      "Epoch: 58 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 22979.53 .. KL_eta: 208.22 .. KL_alpha: 1064898.08 .. Rec_loss: 16956907.36 .. NELBO: 18044992.64\n",
      "****************************************************************************************************\n",
      "Epoch----->58 .. LR: 0.005 .. KL_theta: 22599.84 .. KL_eta: 202.54 .. KL_alpha: 1059429.61 .. Rec_loss: 17028836.06 .. NELBO: 18111067.47\n",
      "****************************************************************************************************\n",
      "Epoch: 59 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 21831.96 .. KL_eta: 225.03 .. KL_alpha: 1053632.62 .. Rec_loss: 17009669.82 .. NELBO: 18085359.64\n",
      "****************************************************************************************************\n",
      "Epoch----->59 .. LR: 0.005 .. KL_theta: 21851.69 .. KL_eta: 232.39 .. KL_alpha: 1046110.31 .. Rec_loss: 16981434.29 .. NELBO: 18049628.88\n",
      "****************************************************************************************************\n",
      "Epoch: 60 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 22656.91 .. KL_eta: 235.3 .. KL_alpha: 1022333.95 .. Rec_loss: 16760477.45 .. NELBO: 17805703.82\n",
      "****************************************************************************************************\n",
      "Epoch----->60 .. LR: 0.005 .. KL_theta: 22524.72 .. KL_eta: 239.15 .. KL_alpha: 1019335.5 .. Rec_loss: 17068123.94 .. NELBO: 18110223.29\n",
      "****************************************************************************************************\n",
      "Epoch: 61 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 22555.1 .. KL_eta: 257.4 .. KL_alpha: 1004805.34 .. Rec_loss: 17066084.36 .. NELBO: 18093702.18\n",
      "****************************************************************************************************\n",
      "Epoch----->61 .. LR: 0.005 .. KL_theta: 22565.22 .. KL_eta: 255.11 .. KL_alpha: 998417.35 .. Rec_loss: 17059526.0 .. NELBO: 18080763.71\n",
      "****************************************************************************************************\n",
      "Epoch: 62 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 22020.01 .. KL_eta: 211.76 .. KL_alpha: 979543.41 .. Rec_loss: 17232533.64 .. NELBO: 18234308.64\n",
      "****************************************************************************************************\n",
      "Epoch----->62 .. LR: 0.005 .. KL_theta: 22375.05 .. KL_eta: 202.46 .. KL_alpha: 975403.93 .. Rec_loss: 17058246.41 .. NELBO: 18056227.65\n",
      "****************************************************************************************************\n",
      "Epoch: 63 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 22588.96 .. KL_eta: 194.32 .. KL_alpha: 956118.99 .. Rec_loss: 17258046.73 .. NELBO: 18236949.36\n",
      "****************************************************************************************************\n",
      "Epoch----->63 .. LR: 0.005 .. KL_theta: 22419.49 .. KL_eta: 193.62 .. KL_alpha: 953768.92 .. Rec_loss: 16917527.59 .. NELBO: 17893909.88\n",
      "****************************************************************************************************\n",
      "Epoch: 64 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 22679.6 .. KL_eta: 194.14 .. KL_alpha: 936806.1 .. Rec_loss: 16983670.18 .. NELBO: 17943350.09\n",
      "****************************************************************************************************\n",
      "Epoch----->64 .. LR: 0.005 .. KL_theta: 22871.74 .. KL_eta: 186.83 .. KL_alpha: 931284.84 .. Rec_loss: 16992119.76 .. NELBO: 17946463.24\n",
      "****************************************************************************************************\n",
      "Epoch: 65 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 22798.52 .. KL_eta: 157.79 .. KL_alpha: 913710.81 .. Rec_loss: 17073620.91 .. NELBO: 18010288.18\n",
      "****************************************************************************************************\n",
      "Epoch----->65 .. LR: 0.005 .. KL_theta: 23237.02 .. KL_eta: 163.78 .. KL_alpha: 912896.21 .. Rec_loss: 17027293.82 .. NELBO: 17963590.82\n",
      "****************************************************************************************************\n",
      "Epoch: 66 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 22716.82 .. KL_eta: 197.41 .. KL_alpha: 894366.33 .. Rec_loss: 17143834.45 .. NELBO: 18061114.73\n",
      "****************************************************************************************************\n",
      "Epoch----->66 .. LR: 0.005 .. KL_theta: 22719.73 .. KL_eta: 199.92 .. KL_alpha: 890845.7 .. Rec_loss: 16929302.41 .. NELBO: 17843067.59\n",
      "****************************************************************************************************\n",
      "Epoch: 67 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 23148.86 .. KL_eta: 191.34 .. KL_alpha: 878750.07 .. Rec_loss: 16918183.64 .. NELBO: 17820273.91\n",
      "****************************************************************************************************\n",
      "Epoch----->67 .. LR: 0.005 .. KL_theta: 23044.68 .. KL_eta: 184.1 .. KL_alpha: 875832.93 .. Rec_loss: 16938560.24 .. NELBO: 17837622.06\n",
      "****************************************************************************************************\n",
      "Epoch: 68 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 23239.1 .. KL_eta: 160.62 .. KL_alpha: 858484.38 .. Rec_loss: 16909134.82 .. NELBO: 17791019.09\n",
      "****************************************************************************************************\n",
      "Epoch----->68 .. LR: 0.005 .. KL_theta: 23187.85 .. KL_eta: 170.7 .. KL_alpha: 854014.23 .. Rec_loss: 16918773.82 .. NELBO: 17796146.82\n",
      "****************************************************************************************************\n",
      "Epoch: 69 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 23729.57 .. KL_eta: 172.55 .. KL_alpha: 838827.62 .. Rec_loss: 16957026.45 .. NELBO: 17819756.36\n",
      "****************************************************************************************************\n",
      "Epoch----->69 .. LR: 0.005 .. KL_theta: 23558.73 .. KL_eta: 180.97 .. KL_alpha: 836634.0 .. Rec_loss: 16960018.41 .. NELBO: 17820392.18\n",
      "****************************************************************************************************\n",
      "Epoch: 70 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 22688.25 .. KL_eta: 209.45 .. KL_alpha: 827749.6 .. Rec_loss: 16961780.55 .. NELBO: 17812427.91\n",
      "****************************************************************************************************\n",
      "Epoch----->70 .. LR: 0.005 .. KL_theta: 23308.22 .. KL_eta: 205.48 .. KL_alpha: 824285.4 .. Rec_loss: 16878289.29 .. NELBO: 17726088.24\n",
      "****************************************************************************************************\n",
      "Epoch: 71 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 23124.22 .. KL_eta: 187.2 .. KL_alpha: 809988.35 .. Rec_loss: 16895617.55 .. NELBO: 17728917.09\n",
      "****************************************************************************************************\n",
      "Epoch----->71 .. LR: 0.005 .. KL_theta: 23175.72 .. KL_eta: 178.11 .. KL_alpha: 807101.19 .. Rec_loss: 16909244.82 .. NELBO: 17739699.65\n",
      "****************************************************************************************************\n",
      "Epoch: 72 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 23592.16 .. KL_eta: 175.3 .. KL_alpha: 796098.51 .. Rec_loss: 16837060.0 .. NELBO: 17656926.45\n",
      "****************************************************************************************************\n",
      "Epoch----->72 .. LR: 0.005 .. KL_theta: 23354.0 .. KL_eta: 170.16 .. KL_alpha: 792647.59 .. Rec_loss: 16904532.94 .. NELBO: 17720705.12\n",
      "****************************************************************************************************\n",
      "Epoch: 73 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 23934.2 .. KL_eta: 163.32 .. KL_alpha: 778766.18 .. Rec_loss: 16939413.64 .. NELBO: 17742277.73\n",
      "****************************************************************************************************\n",
      "Epoch----->73 .. LR: 0.005 .. KL_theta: 23925.47 .. KL_eta: 165.87 .. KL_alpha: 776496.99 .. Rec_loss: 16898670.65 .. NELBO: 17699259.06\n",
      "****************************************************************************************************\n",
      "Epoch: 74 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 23957.84 .. KL_eta: 171.78 .. KL_alpha: 761000.34 .. Rec_loss: 17227022.18 .. NELBO: 18012151.64\n",
      "****************************************************************************************************\n",
      "Epoch----->74 .. LR: 0.005 .. KL_theta: 23722.61 .. KL_eta: 171.03 .. KL_alpha: 758668.18 .. Rec_loss: 16828845.59 .. NELBO: 17611407.12\n",
      "****************************************************************************************************\n",
      "Epoch: 75 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 24200.1 .. KL_eta: 194.44 .. KL_alpha: 750410.94 .. Rec_loss: 16778985.09 .. NELBO: 17553790.18\n",
      "****************************************************************************************************\n",
      "Epoch----->75 .. LR: 0.005 .. KL_theta: 23894.19 .. KL_eta: 194.32 .. KL_alpha: 747093.37 .. Rec_loss: 16910763.24 .. NELBO: 17681944.71\n",
      "****************************************************************************************************\n",
      "Epoch: 76 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 24084.24 .. KL_eta: 157.09 .. KL_alpha: 737328.72 .. Rec_loss: 16933870.45 .. NELBO: 17695440.27\n",
      "****************************************************************************************************\n",
      "Epoch----->76 .. LR: 0.005 .. KL_theta: 24228.22 .. KL_eta: 153.13 .. KL_alpha: 734455.44 .. Rec_loss: 16908245.35 .. NELBO: 17667082.06\n",
      "****************************************************************************************************\n",
      "Epoch: 77 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 23999.69 .. KL_eta: 130.03 .. KL_alpha: 720554.12 .. Rec_loss: 16882787.36 .. NELBO: 17627471.64\n",
      "****************************************************************************************************\n",
      "Epoch----->77 .. LR: 0.005 .. KL_theta: 24108.01 .. KL_eta: 135.17 .. KL_alpha: 715287.37 .. Rec_loss: 16910506.24 .. NELBO: 17650037.29\n",
      "****************************************************************************************************\n",
      "Epoch: 78 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 24174.27 .. KL_eta: 164.66 .. KL_alpha: 705703.13 .. Rec_loss: 16639417.36 .. NELBO: 17369459.18\n",
      "****************************************************************************************************\n",
      "Epoch----->78 .. LR: 0.005 .. KL_theta: 24603.16 .. KL_eta: 163.18 .. KL_alpha: 702909.26 .. Rec_loss: 16854157.35 .. NELBO: 17581832.65\n",
      "****************************************************************************************************\n",
      "Epoch: 79 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 23990.72 .. KL_eta: 170.0 .. KL_alpha: 693301.7 .. Rec_loss: 17069299.82 .. NELBO: 17786762.09\n",
      "****************************************************************************************************\n",
      "Epoch----->79 .. LR: 0.005 .. KL_theta: 24014.17 .. KL_eta: 168.4 .. KL_alpha: 691443.04 .. Rec_loss: 16877289.29 .. NELBO: 17592914.82\n",
      "****************************************************************************************************\n",
      "Epoch: 80 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 24123.71 .. KL_eta: 169.04 .. KL_alpha: 678045.07 .. Rec_loss: 16945297.64 .. NELBO: 17647635.64\n",
      "****************************************************************************************************\n",
      "Epoch----->80 .. LR: 0.005 .. KL_theta: 24215.85 .. KL_eta: 160.84 .. KL_alpha: 676460.66 .. Rec_loss: 16803363.18 .. NELBO: 17504200.76\n",
      "****************************************************************************************************\n",
      "Epoch: 81 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 24240.44 .. KL_eta: 146.78 .. KL_alpha: 667376.49 .. Rec_loss: 16711933.64 .. NELBO: 17403697.45\n",
      "****************************************************************************************************\n",
      "Epoch----->81 .. LR: 0.005 .. KL_theta: 24438.67 .. KL_eta: 146.6 .. KL_alpha: 665919.86 .. Rec_loss: 16896418.59 .. NELBO: 17586923.76\n",
      "****************************************************************************************************\n",
      "Epoch: 82 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 24038.27 .. KL_eta: 151.88 .. KL_alpha: 651245.59 .. Rec_loss: 16878484.18 .. NELBO: 17553919.82\n",
      "****************************************************************************************************\n",
      "Epoch----->82 .. LR: 0.005 .. KL_theta: 24465.96 .. KL_eta: 159.52 .. KL_alpha: 651169.6 .. Rec_loss: 16879472.76 .. NELBO: 17555267.71\n",
      "****************************************************************************************************\n",
      "Epoch: 83 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 24337.14 .. KL_eta: 156.42 .. KL_alpha: 645108.21 .. Rec_loss: 16569105.73 .. NELBO: 17238707.73\n",
      "****************************************************************************************************\n",
      "Epoch----->83 .. LR: 0.005 .. KL_theta: 24579.07 .. KL_eta: 139.42 .. KL_alpha: 643469.05 .. Rec_loss: 16822453.88 .. NELBO: 17490641.47\n",
      "****************************************************************************************************\n",
      "Epoch: 84 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 24710.66 .. KL_eta: 137.36 .. KL_alpha: 633495.77 .. Rec_loss: 16703425.64 .. NELBO: 17361769.45\n",
      "****************************************************************************************************\n",
      "Epoch----->84 .. LR: 0.005 .. KL_theta: 24702.38 .. KL_eta: 143.44 .. KL_alpha: 631641.23 .. Rec_loss: 16821519.41 .. NELBO: 17478006.47\n",
      "****************************************************************************************************\n",
      "Epoch: 85 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 24819.72 .. KL_eta: 145.94 .. KL_alpha: 622087.22 .. Rec_loss: 16503246.27 .. NELBO: 17150299.09\n",
      "****************************************************************************************************\n",
      "Epoch----->85 .. LR: 0.005 .. KL_theta: 24821.97 .. KL_eta: 140.37 .. KL_alpha: 619014.81 .. Rec_loss: 16833137.59 .. NELBO: 17477114.53\n",
      "****************************************************************************************************\n",
      "Epoch: 86 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 25008.9 .. KL_eta: 142.68 .. KL_alpha: 608603.7 .. Rec_loss: 17203474.64 .. NELBO: 17837230.09\n",
      "****************************************************************************************************\n",
      "Epoch----->86 .. LR: 0.005 .. KL_theta: 24727.26 .. KL_eta: 142.23 .. KL_alpha: 607164.44 .. Rec_loss: 16776574.76 .. NELBO: 17408608.71\n",
      "****************************************************************************************************\n",
      "Epoch: 87 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 24884.34 .. KL_eta: 135.79 .. KL_alpha: 599946.53 .. Rec_loss: 17237197.09 .. NELBO: 17862163.18\n",
      "****************************************************************************************************\n",
      "Epoch----->87 .. LR: 0.005 .. KL_theta: 25068.26 .. KL_eta: 138.71 .. KL_alpha: 599256.85 .. Rec_loss: 16770963.06 .. NELBO: 17395426.59\n",
      "****************************************************************************************************\n",
      "Epoch: 88 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 24498.92 .. KL_eta: 131.6 .. KL_alpha: 589867.27 .. Rec_loss: 16772536.55 .. NELBO: 17387034.0\n",
      "****************************************************************************************************\n",
      "Epoch----->88 .. LR: 0.005 .. KL_theta: 24857.58 .. KL_eta: 130.49 .. KL_alpha: 588760.79 .. Rec_loss: 16805576.47 .. NELBO: 17419325.24\n",
      "****************************************************************************************************\n",
      "Epoch: 89 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 24923.13 .. KL_eta: 127.53 .. KL_alpha: 580924.39 .. Rec_loss: 16262360.64 .. NELBO: 16868335.73\n",
      "****************************************************************************************************\n",
      "Epoch----->89 .. LR: 0.005 .. KL_theta: 25425.13 .. KL_eta: 124.52 .. KL_alpha: 580285.42 .. Rec_loss: 16804349.12 .. NELBO: 17410184.18\n",
      "****************************************************************************************************\n",
      "Epoch: 90 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 25302.93 .. KL_eta: 126.74 .. KL_alpha: 572003.24 .. Rec_loss: 16764407.64 .. NELBO: 17361841.09\n",
      "****************************************************************************************************\n",
      "Epoch----->90 .. LR: 0.005 .. KL_theta: 25271.94 .. KL_eta: 129.64 .. KL_alpha: 570833.44 .. Rec_loss: 16753473.29 .. NELBO: 17349708.65\n",
      "****************************************************************************************************\n",
      "Epoch: 91 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 25574.94 .. KL_eta: 142.41 .. KL_alpha: 562067.39 .. Rec_loss: 16740226.36 .. NELBO: 17328011.18\n",
      "****************************************************************************************************\n",
      "Epoch----->91 .. LR: 0.005 .. KL_theta: 25153.21 .. KL_eta: 139.97 .. KL_alpha: 560053.94 .. Rec_loss: 16785922.71 .. NELBO: 17371269.88\n",
      "****************************************************************************************************\n",
      "Epoch: 92 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 26080.79 .. KL_eta: 118.7 .. KL_alpha: 551939.54 .. Rec_loss: 16882655.82 .. NELBO: 17460794.73\n",
      "****************************************************************************************************\n",
      "Epoch----->92 .. LR: 0.005 .. KL_theta: 25699.51 .. KL_eta: 116.0 .. KL_alpha: 551251.42 .. Rec_loss: 16784609.65 .. NELBO: 17361676.59\n",
      "****************************************************************************************************\n",
      "Epoch: 93 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 25703.77 .. KL_eta: 119.66 .. KL_alpha: 542166.74 .. Rec_loss: 16723797.91 .. NELBO: 17291787.73\n",
      "****************************************************************************************************\n",
      "Epoch----->93 .. LR: 0.005 .. KL_theta: 25327.98 .. KL_eta: 127.71 .. KL_alpha: 541132.54 .. Rec_loss: 16779396.53 .. NELBO: 17345984.59\n",
      "****************************************************************************************************\n",
      "Epoch: 94 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 25657.27 .. KL_eta: 136.57 .. KL_alpha: 532201.12 .. Rec_loss: 16770437.73 .. NELBO: 17328432.64\n",
      "****************************************************************************************************\n",
      "Epoch----->94 .. LR: 0.005 .. KL_theta: 25714.39 .. KL_eta: 132.65 .. KL_alpha: 530848.79 .. Rec_loss: 16772974.18 .. NELBO: 17329670.0\n",
      "****************************************************************************************************\n",
      "Epoch: 95 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 25475.86 .. KL_eta: 119.59 .. KL_alpha: 527060.31 .. Rec_loss: 16809192.36 .. NELBO: 17361848.18\n",
      "****************************************************************************************************\n",
      "Epoch----->95 .. LR: 0.005 .. KL_theta: 25406.59 .. KL_eta: 120.51 .. KL_alpha: 524596.87 .. Rec_loss: 16807282.12 .. NELBO: 17357406.24\n",
      "****************************************************************************************************\n",
      "Epoch: 96 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 25724.44 .. KL_eta: 123.96 .. KL_alpha: 516774.36 .. Rec_loss: 16485394.45 .. NELBO: 17028017.0\n",
      "****************************************************************************************************\n",
      "Epoch----->96 .. LR: 0.005 .. KL_theta: 25563.02 .. KL_eta: 121.84 .. KL_alpha: 515907.81 .. Rec_loss: 16790431.12 .. NELBO: 17332023.71\n",
      "****************************************************************************************************\n",
      "Epoch: 97 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 26039.41 .. KL_eta: 109.3 .. KL_alpha: 508980.91 .. Rec_loss: 16785787.45 .. NELBO: 17320917.0\n",
      "****************************************************************************************************\n",
      "Epoch----->97 .. LR: 0.005 .. KL_theta: 25625.49 .. KL_eta: 111.81 .. KL_alpha: 508321.14 .. Rec_loss: 16728983.59 .. NELBO: 17263041.71\n",
      "****************************************************************************************************\n",
      "Epoch: 98 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 26432.48 .. KL_eta: 116.34 .. KL_alpha: 502191.92 .. Rec_loss: 17030769.36 .. NELBO: 17559510.18\n",
      "****************************************************************************************************\n",
      "Epoch----->98 .. LR: 0.005 .. KL_theta: 25683.17 .. KL_eta: 116.19 .. KL_alpha: 501404.19 .. Rec_loss: 16772463.0 .. NELBO: 17299666.47\n",
      "****************************************************************************************************\n",
      "Epoch: 99 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 26486.02 .. KL_eta: 111.68 .. KL_alpha: 491385.62 .. Rec_loss: 16710176.91 .. NELBO: 17228160.45\n",
      "****************************************************************************************************\n",
      "Epoch----->99 .. LR: 0.005 .. KL_theta: 26426.01 .. KL_eta: 112.25 .. KL_alpha: 490698.1 .. Rec_loss: 16801399.06 .. NELBO: 17318635.18\n",
      "****************************************************************************************************\n",
      "saving topic matrix beta...\n",
      "CPU times: user 7h 10min 11s, sys: 21min 10s, total: 7h 31min 22s\n",
      "Wall time: 9min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## train model on data by looping through multiple epochs\n",
    "best_epoch = 0\n",
    "best_val_ppl = 1e9\n",
    "all_val_ppls = []\n",
    "for epoch in range(1, args.epochs):\n",
    "    train(epoch)\n",
    "    # if epoch % args.visualize_every == 0:\n",
    "    #     visualize()\n",
    "    # val_ppl = get_completion_ppl('val')\n",
    "    # print('val_ppl: ', val_ppl)\n",
    "    # if val_ppl < best_val_ppl:\n",
    "    #     with open(ckpt, 'wb') as f:\n",
    "    #         torch.save(model, f)\n",
    "    #     best_epoch = epoch\n",
    "    #     best_val_ppl = val_ppl\n",
    "    # else:\n",
    "    ## check whether to anneal lr\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    if args.anneal_lr and (len(all_val_ppls) > args.nonmono and val_ppl > min(all_val_ppls[:-args.nonmono]) and lr > 1e-5):\n",
    "        optimizer.param_groups[0]['lr'] /= args.lr_factor\n",
    "    #all_val_ppls.append(val_ppl)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('saving topic matrix beta...')\n",
    "    alpha = model.mu_q_alpha\n",
    "    beta = model.get_beta(alpha).cpu().numpy()\n",
    "    scipy.io.savemat(ckpt+'_beta.mat', {'values': beta}, do_compression=True)\n",
    "    if args.train_embeddings:\n",
    "        print('saving word embedding matrix rho...')\n",
    "        rho = model.rho.weight.cpu().numpy()\n",
    "        scipy.io.savemat(ckpt+'_rho.mat', {'values': rho}, do_compression=True)\n",
    "    # print('computing validation perplexity...')\n",
    "    # val_ppl = get_completion_ppl('val')\n",
    "    # print('computing test perplexity...')\n",
    "    # test_ppl = get_completion_ppl('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f1e2940c-e1ee-4610-87cf-c02b84767760",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff7e94ff-629a-49ca-b780-df360a861b91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta:  torch.Size([10, 4, 4938])\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Visualize topics...\n",
      "Topic 0 .. Time: 0 ===> ['proactive', 'model', 'antidote', 'summarize', 'adaptive', 'criticize', 'stringent', 'pause', 'naturally']\n",
      "Topic 0 .. Time: 2 ===> ['model', 'proactive', 'adaptive', 'antidote', 'proceeding', 'summarize', '1920', 'criticize', 'manipulation']\n",
      "Topic 1 .. Time: 0 ===> ['verify', 'vice', 'concerted', 'profitably', 'respect', 'memorial', 'copper', 'ecb', 'denominate']\n",
      "Topic 1 .. Time: 2 ===> ['verify', 'concerted', 'profitably', 'vice', 'copper', 'innovator', 'ecb', 'stride', 'fruitful']\n",
      "Topic 2 .. Time: 0 ===> ['entire', 'correlation', 'master', 'impending', 'climb', 'experience', 'bulk', 'government', 'erosion']\n",
      "Topic 2 .. Time: 2 ===> ['correlation', 'climb', 'impending', 'master', 'prerequisite', 'bulk', 'sudden', 'entire', 'erosion']\n",
      "Topic 3 .. Time: 0 ===> ['crisis', 'memorial', 'overwhelming', 'burden', 'engaged', 'skepticism', 'detrimental', 'barely', 'possibly']\n",
      "Topic 3 .. Time: 2 ===> ['damage', 'overwhelming', 'crisis', 'memorial', 'ordinary', 'fullscreen', 'possibly', 'engaged', 'chart']\n",
      "Topic 4 .. Time: 0 ===> ['fulfil', 'friend', 'encompass', 'legitimate', 'birth', 'immense', 'statistic', 'pickup', 'mitigation']\n",
      "Topic 4 .. Time: 2 ===> ['probable', 'encompass', 'prepare', 'immense', 'persistent', 'fulfil', 'friend', 'conservation', 'akin']\n",
      "Topic 5 .. Time: 0 ===> ['unsettled', 'revitalization', 'call', 'productive', 'model', 'president', 'battle', 'reassessment', 'fulfil']\n",
      "Topic 5 .. Time: 2 ===> ['unsettled', 'revitalization', 'model', 'call', 'president', 'contribute', 'productive', 'sounder', 'battle']\n",
      "Topic 6 .. Time: 0 ===> ['fulfil', 'subordinate', 'introductory', 'clearing', 'native', 'manager', 'crude', 'revitalization', 'author']\n",
      "Topic 6 .. Time: 2 ===> ['fulfil', 'native', 'clearing', 'introductory', 'author', 'manager', 'specialization', 'insurmountable', 'dispose']\n",
      "Topic 7 .. Time: 0 ===> ['manipulation', 'comprehend', 'license', 'positively', 'unit', 'purely', 'peak', 'harness', 'burden']\n",
      "Topic 7 .. Time: 2 ===> ['manipulation', 'navigate', 'comprehend', 'license', 'memorial', 'harness', 'ecb', 'narrative', 'purely']\n",
      "Topic 8 .. Time: 0 ===> ['proceeding', 'contentious', 'appeal', 'barely', 'denominate', 'pause', 'detrimental', 'wage', 'inequality']\n",
      "Topic 8 .. Time: 2 ===> ['proceeding', 'contentious', 'inequality', 'barely', 'detrimental', 'appeal', 'denominate', 'explain', 'simplified']\n",
      "Topic 9 .. Time: 0 ===> ['quarterly', 'invite', 'indirectly', 'politician', 'transmit', 'luck', 'entrepreneurship', 'fiber', 'audit']\n",
      "Topic 9 .. Time: 2 ===> ['quarterly', 'invite', 'indirectly', 'transmit', 'politician', 'fiber', 'pose', 'luck', 'brian']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    alpha = model.mu_q_alpha\n",
    "    beta = model.get_beta(alpha) \n",
    "    print('beta: ', beta.size())\n",
    "    print('\\n')\n",
    "    print('#'*100)\n",
    "    print('Visualize topics...')\n",
    "    times = [0, 2]\n",
    "    topics_words = []\n",
    "    for k in range(args.num_topics):\n",
    "        for t in times:\n",
    "            gamma = beta[k, t, :]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-args.num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words]\n",
    "            topics_words.append(' '.join(topic_words))\n",
    "            print('Topic {} .. Time: {} ===> {}'.format(k, t, topic_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "150f47da-b071-43d7-b14e-2616fe9f84a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_topic_coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89a03418-f756-4b14-a23f-d571f6bf60b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta:  torch.Size([10, 4, 4938])\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Get topic diversity...\n",
      "Topic Diversity is: 0.7849999999999999\n",
      "\n",
      "\n",
      "Get topic coherence...\n",
      "train_tokens:  [  53   81  223  315  374  630  752  828 1104 1106 1142 1405 1511 1639\n",
      " 1642 1698 1743 1780 1948 2153 2231 2367 2797 2825 2826 3006 3030 3251\n",
      " 3273 3357 3503 3553 3752 4026 4081 4091 4185 4363 4398 4456 4512]\n",
      "D:  1650\n",
      "k: 0/10\n",
      "k: 1/10\n",
      "k: 2/10\n",
      "k: 3/10\n",
      "k: 4/10\n",
      "k: 5/10\n",
      "k: 6/10\n",
      "k: 7/10\n",
      "k: 8/10\n",
      "k: 9/10\n",
      "counter:  55\n",
      "num topics:  10\n",
      "Topic Coherence is: [4.2989044858211765, 5.444078348380458, 5.494838475015099, 4.9122535477502955, 5.637867237361495, 4.212042889671402, 3.92523855649542, 6.5435163527632625, 6.894564392415401, 4.829185300933675]\n",
      "D:  1650\n",
      "k: 0/10\n",
      "k: 1/10\n",
      "k: 2/10\n",
      "k: 3/10\n",
      "k: 4/10\n",
      "k: 5/10\n",
      "k: 6/10\n",
      "k: 7/10\n",
      "k: 8/10\n",
      "k: 9/10\n",
      "counter:  55\n",
      "num topics:  10\n",
      "Topic Coherence is: [6.003318624617093, 5.343364194291605, 4.903805150637484, 4.5720706152579265, 2.584586181830555, 6.3913640535894345, 6.391411809518675, 3.0600062621944306, 9.376004138410172, 4.409398554403079]\n",
      "D:  1650\n",
      "k: 0/10\n",
      "k: 1/10\n",
      "k: 2/10\n",
      "k: 3/10\n",
      "k: 4/10\n",
      "k: 5/10\n",
      "k: 6/10\n",
      "k: 7/10\n",
      "k: 8/10\n",
      "k: 9/10\n",
      "counter:  55\n",
      "num topics:  10\n",
      "Topic Coherence is: [2.182192659990314, 3.890746788098359, 3.93785060152958, 5.5782899399208725, 5.879559559766985, 5.4957472718019815, 2.7799222524690546, 3.415613999557088, 7.293284027055671, 3.18720307463993]\n",
      "D:  1650\n",
      "k: 0/10\n",
      "k: 1/10\n",
      "k: 2/10\n",
      "k: 3/10\n",
      "k: 4/10\n",
      "k: 5/10\n",
      "k: 6/10\n",
      "k: 7/10\n",
      "k: 8/10\n",
      "k: 9/10\n",
      "counter:  55\n",
      "num topics:  10\n",
      "Topic Coherence is: [2.434415837604901, 2.865114663168448, 1.1180832260182516, 4.411907969433501, 1.2874479814020412, 5.836073499512658, 2.860013939686321, 3.0081982448810436, 2.774794432136924, 3.0961692076026335]\n",
      "TC_all:  [[4.2989044858211765, 5.444078348380458, 5.494838475015099, 4.9122535477502955, 5.637867237361495, 4.212042889671402, 3.92523855649542, 6.5435163527632625, 6.894564392415401, 4.829185300933675], [6.003318624617093, 5.343364194291605, 4.903805150637484, 4.5720706152579265, 2.584586181830555, 6.3913640535894345, 6.391411809518675, 3.0600062621944306, 9.376004138410172, 4.409398554403079], [2.182192659990314, 3.890746788098359, 3.93785060152958, 5.5782899399208725, 5.879559559766985, 5.4957472718019815, 2.7799222524690546, 3.415613999557088, 7.293284027055671, 3.18720307463993], [2.434415837604901, 2.865114663168448, 1.1180832260182516, 4.411907969433501, 1.2874479814020412, 5.836073499512658, 2.860013939686321, 3.0081982448810436, 2.774794432136924, 3.0961692076026335]]\n",
      "TC_all:  torch.Size([4, 10])\n",
      "\n",
      "\n",
      "Get topic quality...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'numpy.float64'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGet topic quality...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m quality \u001b[38;5;241m=\u001b[39m \u001b[43mtc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTD\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTopic Quality is: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(quality))\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'numpy.float64'"
     ]
    }
   ],
   "source": [
    "def _diversity_helper(beta, num_tops):\n",
    "    list_w = np.zeros((args.num_topics, num_tops))\n",
    "    for k in range(args.num_topics):\n",
    "        gamma = beta[k, :]\n",
    "        top_words = gamma.cpu().numpy().argsort()[-num_tops:][::-1]\n",
    "        list_w[k, :] = top_words\n",
    "    list_w = np.reshape(list_w, (-1))\n",
    "    list_w = list(list_w)\n",
    "    n_unique = len(np.unique(list_w))\n",
    "    diversity = n_unique / (args.num_topics * num_tops)\n",
    "    return diversity\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    alpha = model.mu_q_alpha\n",
    "    beta = model.get_beta(alpha) \n",
    "    print('beta: ', beta.size())\n",
    "\n",
    "    print('\\n')\n",
    "    print('#'*100)\n",
    "    print('Get topic diversity...')\n",
    "    num_tops = 25\n",
    "    TD_all = np.zeros((args.num_times,))\n",
    "    for tt in range(args.num_times):\n",
    "        TD_all[tt] = _diversity_helper(beta[:, tt, :], num_tops)\n",
    "    TD = np.mean(TD_all)\n",
    "    print('Topic Diversity is: {}'.format(TD))\n",
    "\n",
    "    print('\\n')\n",
    "    print('Get topic coherence...')\n",
    "    print('train_tokens: ', train_tokens[0])\n",
    "    TC_all = []\n",
    "    cnt_all = []\n",
    "    for tt in range(args.num_times):\n",
    "        tc, cnt = get_topic_coherence(beta[:, tt, :].cpu().numpy(), train_tokens, vocab)\n",
    "        TC_all.append(tc)\n",
    "        cnt_all.append(cnt)\n",
    "    print('TC_all: ', TC_all)\n",
    "    TC_all = torch.tensor(TC_all)\n",
    "    print('TC_all: ', TC_all.size())\n",
    "    print('\\n')\n",
    "    print('Get topic quality...')\n",
    "    quality = tc * TD\n",
    "    print('Topic Quality is: {}'.format(quality))\n",
    "    print('#'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60ce5b-33c9-46d5-8052-4011e8ff7559",
   "metadata": {},
   "source": [
    "## see if i can get a measure of topic coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9e47996a-e7d0-4d97-945c-6929968fd00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = Dictionary.load('dict_save')\n",
    "df = pd.read_parquet('data/combined_clean.parquet')\n",
    "split_text = df['filtered_text'].str.split().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a57dc5f3-a272-4f14-a5f3-39267c9da244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['proactive', 'model', 'antidote', 'summarize', 'adaptive', 'criticize', 'stringent', 'pause', 'naturally', 'compelling']\n",
      "['verify', 'vice', 'concerted', 'profitably', 'respect', 'memorial', 'copper', 'ecb', 'denominate', 'college']\n",
      "['entire', 'correlation', 'master', 'impending', 'climb', 'experience', 'bulk', 'government', 'erosion', 'prerequisite']\n",
      "['crisis', 'memorial', 'overwhelming', 'burden', 'engaged', 'skepticism', 'detrimental', 'barely', 'possibly', 'virtuous']\n",
      "['fulfil', 'friend', 'encompass', 'legitimate', 'birth', 'immense', 'statistic', 'pickup', 'mitigation', 'akin']\n",
      "['unsettled', 'revitalization', 'call', 'productive', 'model', 'president', 'battle', 'reassessment', 'fulfil', 'sounder']\n",
      "['fulfil', 'subordinate', 'introductory', 'clearing', 'native', 'manager', 'crude', 'revitalization', 'author', 'specifically']\n",
      "['manipulation', 'comprehend', 'license', 'positively', 'unit', 'purely', 'peak', 'harness', 'burden', 'ecb']\n",
      "['proceeding', 'contentious', 'appeal', 'barely', 'denominate', 'pause', 'detrimental', 'wage', 'inequality', 'classify']\n",
      "['quarterly', 'invite', 'indirectly', 'politician', 'transmit', 'luck', 'entrepreneurship', 'fiber', 'audit', 'pose']\n"
     ]
    }
   ],
   "source": [
    "for topic in get_detm_topics(model=model, time=0, num_words=10, vocab=vocab, num_topics=10):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "55e5a6f0-9fec-4550-9b17-0e536c9595bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:43<00:00, 10.88s/it]\n"
     ]
    }
   ],
   "source": [
    "coherences = []\n",
    "for t in tqdm(range(args.num_times)):\n",
    "    coherences.append(\n",
    "        CoherenceModel(\n",
    "            topics=get_detm_topics(model=model, time=t, num_words=20, vocab=vocab, num_topics=args.num_topics), # use 20 words to standardize with DTM\n",
    "            texts=split_text, \n",
    "            dictionary=id2word, \n",
    "            coherence='c_v'\n",
    "        ).get_coherence()\n",
    "    )\n",
    "\n",
    "coherences = np.array(coherences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "46a4b7b1-7a78-4d78-b42c-6845c032bb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:00<00:00, 180.88it/s]\n"
     ]
    }
   ],
   "source": [
    "diversities = []\n",
    "for t in tqdm(range(args.num_times)):\n",
    "    diversities.append(\n",
    "        topic_diversity(topics=get_topics(model=model, time=t, num_words=20, vocab=vocab, num_topics=args.num_topics))\n",
    "    )\n",
    "    \n",
    "diversities = np.array(diversities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a1a4184d-93eb-4844-87c7-174d5ca8f485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.43268138217844765, 0.008716614120983622)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qualities = diversities * coherences\n",
    "qualities.mean(), qualities.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6294257d-5fc9-45d2-aab7-c57b24a99af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42344675, 0.44412863, 0.43810009, 0.42505006])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01cf7948-c9df-454d-82e1-8c1b95b787ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\n",
    "    'detm_stats.npz',\n",
    "    coherence=coherences,\n",
    "    diversity=diversities\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270367e8-10cf-4ef4-8674-d9f25af4971e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
