{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293077f9-46f1-4489-b8ac-b2de4bef194a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs224n_a3/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from DETM import data\n",
    "from DETM.detm import DETM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1309530f-89db-4946-b9ea-bce4cc36ea74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "680dc701-a07d-46a5-9cc4-d351bc25fef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### data and file related arguments\n",
    "arg_str = \"\"\"\n",
    "parser.add_argument('--dataset', type=str, default='un', help='name of corpus')\n",
    "parser.add_argument('--data_path', type=str, default='un/', help='directory containing data')\n",
    "parser.add_argument('--emb_path', type=str, default='skipgram/embeddings.txt', help='directory containing embeddings')\n",
    "parser.add_argument('--save_path', type=str, default='./results', help='path to save results')\n",
    "parser.add_argument('--batch_size', type=int, default=1000, help='number of documents in a batch for training')\n",
    "parser.add_argument('--min_df', type=int, default=100, help='to get the right data..minimum document frequency')\n",
    "\n",
    "### model-related arguments\n",
    "parser.add_argument('--num_topics', type=int, default=50, help='number of topics')\n",
    "parser.add_argument('--rho_size', type=int, default=300, help='dimension of rho')\n",
    "parser.add_argument('--emb_size', type=int, default=300, help='dimension of embeddings')\n",
    "parser.add_argument('--t_hidden_size', type=int, default=800, help='dimension of hidden space of q(theta)')\n",
    "parser.add_argument('--theta_act', type=str, default='relu', help='tanh, softplus, relu, rrelu, leakyrelu, elu, selu, glu)')\n",
    "parser.add_argument('--train_embeddings', type=int, default=1, help='whether to fix rho or train it')\n",
    "parser.add_argument('--eta_nlayers', type=int, default=3, help='number of layers for eta')\n",
    "parser.add_argument('--eta_hidden_size', type=int, default=200, help='number of hidden units for rnn')\n",
    "parser.add_argument('--delta', type=float, default=0.005, help='prior variance')\n",
    "\n",
    "### optimization-related arguments\n",
    "parser.add_argument('--lr', type=float, default=0.005, help='learning rate')\n",
    "parser.add_argument('--lr_factor', type=float, default=4.0, help='divide learning rate by this')\n",
    "parser.add_argument('--epochs', type=int, default=100, help='number of epochs to train')\n",
    "parser.add_argument('--mode', type=str, default='train', help='train or eval model')\n",
    "parser.add_argument('--optimizer', type=str, default='adam', help='choice of optimizer')\n",
    "parser.add_argument('--seed', type=int, default=2019, help='random seed (default: 1)')\n",
    "parser.add_argument('--enc_drop', type=float, default=0.0, help='dropout rate on encoder')\n",
    "parser.add_argument('--eta_dropout', type=float, default=0.0, help='dropout rate on rnn for eta')\n",
    "parser.add_argument('--clip', type=float, default=0.0, help='gradient clipping')\n",
    "parser.add_argument('--nonmono', type=int, default=10, help='number of bad hits allowed')\n",
    "parser.add_argument('--wdecay', type=float, default=1.2e-6, help='some l2 regularization')\n",
    "parser.add_argument('--anneal_lr', type=int, default=0, help='whether to anneal the learning rate or not')\n",
    "parser.add_argument('--bow_norm', type=int, default=1, help='normalize the bows or not')\n",
    "\n",
    "### evaluation, visualization, and logging-related arguments\n",
    "parser.add_argument('--num_words', type=int, default=20, help='number of words for topic viz')\n",
    "parser.add_argument('--log_interval', type=int, default=10, help='when to log training')\n",
    "parser.add_argument('--visualize_every', type=int, default=1, help='when to visualize results')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=1000, help='input batch size for evaluation')\n",
    "parser.add_argument('--load_from', type=str, default='', help='the name of the ckpt to eval from')\n",
    "parser.add_argument('--tc', type=int, default=0, help='whether to compute tc or not')\n",
    "\"\"\".split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c037c86-db04-4fa8-8fc5-13f99332813d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keys = [x.strip(\"parser.add_argument('\").split(',')[0].strip('--').strip(\"'\") for x in arg_str if (len(x) > 0) and (not x.startswith('#'))]\n",
    "values = [x.strip(\"parser.add_argument('\").split(',')[2].strip(\" default=\").strip(\"'\") for x in arg_str if (len(x) > 0) and (not x.startswith('#'))]\n",
    "tmp_dict = dict(zip(keys, values))\n",
    "\n",
    "for k, v in tmp_dict.items():\n",
    "    if v.isnumeric():\n",
    "        tmp_dict[k] = int(v)\n",
    "    elif ('.' in v) and (v[0].isnumeric()):\n",
    "        tmp_dict[k] = float(v)    \n",
    "\n",
    "args = AttrDict()\n",
    "args.update(tmp_dict)\n",
    "\n",
    "args.train_embeddings = 0\n",
    "args.rho_size = 96\n",
    "args.num_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "574d5721-f212-49cf-afe9-dddf97ee08d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_arr = np.load('test_data.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e20155d7-2fff-4d89-a127-ba89a66d01f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_tokens = train_arr['train_tokens']\n",
    "train_counts = train_arr['train_counts']\n",
    "train_times = train_arr['train_times']\n",
    "vocab = train_arr['vocab']\n",
    "embeddings = train_arr['embeddings']\n",
    "\n",
    "args.num_times = len(np.unique(train_times))\n",
    "args.num_docs_train = len(train_tokens)\n",
    "args.vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "21ada37c-a91d-4293-a147-7de92607ae8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0/2\n",
      "CPU times: user 1.01 s, sys: 263 ms, total: 1.27 s\n",
      "Wall time: 364 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_rnn_inp = data.get_rnn_input(train_tokens, train_counts, train_times, args.num_times, args.vocab_size, args.num_docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c6b0df7f-8ffb-4dea-84c5-3faa403ac5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(args['save_path']):\n",
    "    os.makedirs(args['save_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "24c7d9fa-102e-4d7e-8734-53096816d0a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if args.mode == 'eval':\n",
    "    ckpt = args.load_from\n",
    "else:\n",
    "    ckpt = os.path.join(args.save_path, \n",
    "        'detm_{}_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_L_{}_minDF_{}_trainEmbeddings_{}'.format(\n",
    "        args.dataset, args.num_topics, args.t_hidden_size, args.optimizer, args.clip, args.theta_act, \n",
    "            args.lr, args.batch_size, args.rho_size, args.eta_nlayers, args.min_df, args.train_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "047e40c2-92aa-4707-a77c-3db9313831cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DETM(\n",
       "  (t_drop): Dropout(p=0.0, inplace=False)\n",
       "  (theta_act): ReLU()\n",
       "  (q_theta): Sequential(\n",
       "    (0): Linear(in_features=25307, out_features=800, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (mu_q_theta): Linear(in_features=800, out_features=10, bias=True)\n",
       "  (logsigma_q_theta): Linear(in_features=800, out_features=10, bias=True)\n",
       "  (q_eta_map): Linear(in_features=25297, out_features=200, bias=True)\n",
       "  (q_eta): LSTM(200, 200, num_layers=3)\n",
       "  (mu_q_eta): Linear(in_features=210, out_features=10, bias=True)\n",
       "  (logsigma_q_eta): Linear(in_features=210, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embeddings = torch.from_numpy(embeddings).to(device)\n",
    "args.embeddings_dim = embeddings.size()\n",
    "\n",
    "model = DETM(args, embeddings)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f54de551-16ca-419d-876e-1ba965d39d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if args.optimizer == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'adadelta':\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'asgd':\n",
    "    optimizer = optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\n",
    "else:\n",
    "    print('Defaulting to vanilla SGD')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "71a92f90-5e83-4bf4-a3a0-2d9390061275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    \"\"\"Train DETM on data for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    acc_loss = 0\n",
    "    acc_nll = 0\n",
    "    acc_kl_theta_loss = 0\n",
    "    acc_kl_eta_loss = 0\n",
    "    acc_kl_alpha_loss = 0\n",
    "    cnt = 0\n",
    "    indices = torch.randperm(args.num_docs_train)\n",
    "    indices = torch.split(indices, args.batch_size) \n",
    "    for idx, ind in enumerate(indices):\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        data_batch, times_batch = data.get_batch(\n",
    "            train_tokens, train_counts, ind, args.vocab_size, args.emb_size, temporal=True, times=train_times)\n",
    "        sums = data_batch.sum(1).unsqueeze(1)\n",
    "        if args.bow_norm:\n",
    "            normalized_data_batch = data_batch / sums\n",
    "        else:\n",
    "            normalized_data_batch = data_batch\n",
    "\n",
    "        loss, nll, kl_alpha, kl_eta, kl_theta = model(data_batch, normalized_data_batch, times_batch, train_rnn_inp, args.num_docs_train)\n",
    "        loss.backward()\n",
    "        if args.clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        acc_loss += torch.sum(loss).item()\n",
    "        acc_nll += torch.sum(nll).item()\n",
    "        acc_kl_theta_loss += torch.sum(kl_theta).item()\n",
    "        acc_kl_eta_loss += torch.sum(kl_eta).item()\n",
    "        acc_kl_alpha_loss += torch.sum(kl_alpha).item()\n",
    "        cnt += 1\n",
    "\n",
    "        if idx % args.log_interval == 0 and idx > 0:\n",
    "            cur_loss = round(acc_loss / cnt, 2) \n",
    "            cur_nll = round(acc_nll / cnt, 2) \n",
    "            cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "            cur_kl_eta = round(acc_kl_eta_loss / cnt, 2) \n",
    "            cur_kl_alpha = round(acc_kl_alpha_loss / cnt, 2) \n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            print('Epoch: {} .. batch: {}/{} .. LR: {} .. KL_theta: {} .. KL_eta: {} .. KL_alpha: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "                epoch, idx, len(indices), lr, cur_kl_theta, cur_kl_eta, cur_kl_alpha, cur_nll, cur_loss))\n",
    "    \n",
    "    cur_loss = round(acc_loss / cnt, 2) \n",
    "    cur_nll = round(acc_nll / cnt, 2) \n",
    "    cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "    cur_kl_eta = round(acc_kl_eta_loss / cnt, 2) \n",
    "    cur_kl_alpha = round(acc_kl_alpha_loss / cnt, 2) \n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    print('*'*100)\n",
    "    print('Epoch----->{} .. LR: {} .. KL_theta: {} .. KL_eta: {} .. KL_alpha: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "            epoch, lr, cur_kl_theta, cur_kl_eta, cur_kl_alpha, cur_nll, cur_loss))\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b51ea22c-39c7-4ece-a26c-2c97c817146f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 8642.07 .. KL_eta: 8276.83 .. KL_alpha: 2041300.38 .. Rec_loss: 33503111.0 .. NELBO: 35561330.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 4637.09 .. KL_eta: 3162.51 .. KL_alpha: 2100889.69 .. Rec_loss: 33352823.0 .. NELBO: 35461512.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 5470.14 .. KL_eta: 744.08 .. KL_alpha: 2049901.12 .. Rec_loss: 33640958.0 .. NELBO: 35697074.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 3668.65 .. KL_eta: 253.26 .. KL_alpha: 1990775.69 .. Rec_loss: 33461623.0 .. NELBO: 35456320.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 2891.0 .. KL_eta: 136.44 .. KL_alpha: 1975144.81 .. Rec_loss: 33390181.0 .. NELBO: 35368352.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 7254.39 .. KL_eta: 91.35 .. KL_alpha: 1941132.44 .. Rec_loss: 33484214.0 .. NELBO: 35432692.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 3053.09 .. KL_eta: 75.27 .. KL_alpha: 1954644.62 .. Rec_loss: 33354250.0 .. NELBO: 35312026.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 5041.99 .. KL_eta: 53.39 .. KL_alpha: 1945332.94 .. Rec_loss: 33428961.0 .. NELBO: 35379388.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 6473.53 .. KL_eta: 68.84 .. KL_alpha: 1903553.5 .. Rec_loss: 33455310.0 .. NELBO: 35365404.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 7123.33 .. KL_eta: 61.66 .. KL_alpha: 1892054.88 .. Rec_loss: 33540967.0 .. NELBO: 35440208.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 7465.08 .. KL_eta: 71.61 .. KL_alpha: 1929321.88 .. Rec_loss: 33546390.0 .. NELBO: 35483250.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 5569.83 .. KL_eta: 85.46 .. KL_alpha: 1849353.31 .. Rec_loss: 33464739.0 .. NELBO: 35319746.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 3099.56 .. KL_eta: 86.31 .. KL_alpha: 1851306.75 .. Rec_loss: 33441677.0 .. NELBO: 35296170.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 1927.16 .. KL_eta: 79.47 .. KL_alpha: 1853604.44 .. Rec_loss: 33408802.0 .. NELBO: 35264414.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 1557.6 .. KL_eta: 78.14 .. KL_alpha: 1823489.69 .. Rec_loss: 33438786.0 .. NELBO: 35263912.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 3684.85 .. KL_eta: 68.96 .. KL_alpha: 1826473.44 .. Rec_loss: 33438100.0 .. NELBO: 35268328.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 6097.3 .. KL_eta: 66.72 .. KL_alpha: 1800731.5 .. Rec_loss: 33392688.0 .. NELBO: 35199582.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 4301.12 .. KL_eta: 69.69 .. KL_alpha: 1808767.62 .. Rec_loss: 33628956.0 .. NELBO: 35442094.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 3252.33 .. KL_eta: 70.36 .. KL_alpha: 1795764.0 .. Rec_loss: 33332209.0 .. NELBO: 35131298.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 4193.39 .. KL_eta: 84.27 .. KL_alpha: 1740568.94 .. Rec_loss: 33564366.0 .. NELBO: 35309214.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 5660.15 .. KL_eta: 107.24 .. KL_alpha: 1754822.81 .. Rec_loss: 33389805.0 .. NELBO: 35150394.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 6079.35 .. KL_eta: 115.71 .. KL_alpha: 1752857.75 .. Rec_loss: 33653902.0 .. NELBO: 35412954.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 4293.15 .. KL_eta: 103.94 .. KL_alpha: 1762817.12 .. Rec_loss: 33468322.0 .. NELBO: 35235536.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 4016.83 .. KL_eta: 98.34 .. KL_alpha: 1695667.0 .. Rec_loss: 33178828.0 .. NELBO: 34878610.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 4245.84 .. KL_eta: 85.73 .. KL_alpha: 1684998.94 .. Rec_loss: 33491025.0 .. NELBO: 35180356.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 5118.61 .. KL_eta: 83.77 .. KL_alpha: 1699417.19 .. Rec_loss: 33086023.0 .. NELBO: 34790642.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 5344.59 .. KL_eta: 75.88 .. KL_alpha: 1664111.56 .. Rec_loss: 33323136.0 .. NELBO: 34992668.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 5673.03 .. KL_eta: 87.12 .. KL_alpha: 1706423.25 .. Rec_loss: 33571488.0 .. NELBO: 35283670.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 5086.08 .. KL_eta: 102.28 .. KL_alpha: 1636934.94 .. Rec_loss: 33318398.0 .. NELBO: 34960520.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 3759.89 .. KL_eta: 106.52 .. KL_alpha: 1676865.94 .. Rec_loss: 33516776.0 .. NELBO: 35197508.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 3562.37 .. KL_eta: 95.43 .. KL_alpha: 1646010.12 .. Rec_loss: 33511844.0 .. NELBO: 35161512.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 4179.9 .. KL_eta: 97.34 .. KL_alpha: 1626585.81 .. Rec_loss: 33358690.0 .. NELBO: 34989552.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 5163.49 .. KL_eta: 103.38 .. KL_alpha: 1574731.75 .. Rec_loss: 33469919.0 .. NELBO: 35049918.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 5025.47 .. KL_eta: 102.3 .. KL_alpha: 1588692.0 .. Rec_loss: 33541714.0 .. NELBO: 35135534.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 5074.13 .. KL_eta: 96.87 .. KL_alpha: 1615294.06 .. Rec_loss: 33461851.0 .. NELBO: 35082318.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 5274.39 .. KL_eta: 97.33 .. KL_alpha: 1582754.19 .. Rec_loss: 33273239.0 .. NELBO: 34861362.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 5643.7 .. KL_eta: 93.46 .. KL_alpha: 1572010.94 .. Rec_loss: 33355880.0 .. NELBO: 34933628.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 6204.51 .. KL_eta: 75.45 .. KL_alpha: 1567165.25 .. Rec_loss: 33242152.0 .. NELBO: 34815598.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 6579.5 .. KL_eta: 86.39 .. KL_alpha: 1558316.0 .. Rec_loss: 33382929.0 .. NELBO: 34947910.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 6785.85 .. KL_eta: 84.42 .. KL_alpha: 1584911.75 .. Rec_loss: 33353766.0 .. NELBO: 34945548.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 4584.1 .. KL_eta: 91.0 .. KL_alpha: 1540057.56 .. Rec_loss: 33447641.0 .. NELBO: 34992372.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 4164.32 .. KL_eta: 85.89 .. KL_alpha: 1505629.94 .. Rec_loss: 33343878.0 .. NELBO: 34853758.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 7322.49 .. KL_eta: 91.97 .. KL_alpha: 1523219.88 .. Rec_loss: 33402215.0 .. NELBO: 34932850.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 8656.06 .. KL_eta: 89.85 .. KL_alpha: 1503887.75 .. Rec_loss: 33363601.0 .. NELBO: 34876236.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 8713.87 .. KL_eta: 80.13 .. KL_alpha: 1508242.12 .. Rec_loss: 33444998.0 .. NELBO: 34962034.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 12257.9 .. KL_eta: 122.3 .. KL_alpha: 1468871.5 .. Rec_loss: 33227991.0 .. NELBO: 34709244.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 10510.79 .. KL_eta: 186.65 .. KL_alpha: 1497944.75 .. Rec_loss: 33396122.0 .. NELBO: 34904764.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 8740.42 .. KL_eta: 450.41 .. KL_alpha: 1455736.75 .. Rec_loss: 33198112.0 .. NELBO: 34663038.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 9925.52 .. KL_eta: 842.98 .. KL_alpha: 1452571.25 .. Rec_loss: 33139913.0 .. NELBO: 34603254.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 12966.61 .. KL_eta: 1150.28 .. KL_alpha: 1447804.5 .. Rec_loss: 33262441.0 .. NELBO: 34724362.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->51 .. LR: 0.005 .. KL_theta: 12007.23 .. KL_eta: 835.37 .. KL_alpha: 1447699.38 .. Rec_loss: 33265628.0 .. NELBO: 34726170.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->52 .. LR: 0.005 .. KL_theta: 13999.41 .. KL_eta: 497.46 .. KL_alpha: 1419219.38 .. Rec_loss: 33447515.0 .. NELBO: 34881232.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->53 .. LR: 0.005 .. KL_theta: 14194.03 .. KL_eta: 307.25 .. KL_alpha: 1445166.38 .. Rec_loss: 33349717.0 .. NELBO: 34809386.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->54 .. LR: 0.005 .. KL_theta: 6422.61 .. KL_eta: 187.02 .. KL_alpha: 1439371.88 .. Rec_loss: 33006174.0 .. NELBO: 34452156.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->55 .. LR: 0.005 .. KL_theta: 25568.43 .. KL_eta: 159.22 .. KL_alpha: 1390133.0 .. Rec_loss: 33198530.0 .. NELBO: 34614390.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->56 .. LR: 0.005 .. KL_theta: 11887.35 .. KL_eta: 141.35 .. KL_alpha: 1415915.62 .. Rec_loss: 33349377.0 .. NELBO: 34777320.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->57 .. LR: 0.005 .. KL_theta: 4701.62 .. KL_eta: 188.32 .. KL_alpha: 1394868.38 .. Rec_loss: 33062327.0 .. NELBO: 34462086.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->58 .. LR: 0.005 .. KL_theta: 6749.42 .. KL_eta: 297.89 .. KL_alpha: 1359558.5 .. Rec_loss: 33283151.0 .. NELBO: 34649756.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->59 .. LR: 0.005 .. KL_theta: 10008.53 .. KL_eta: 391.87 .. KL_alpha: 1388673.31 .. Rec_loss: 33163027.0 .. NELBO: 34562100.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->60 .. LR: 0.005 .. KL_theta: 10309.67 .. KL_eta: 447.67 .. KL_alpha: 1369817.31 .. Rec_loss: 33248183.0 .. NELBO: 34628758.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->61 .. LR: 0.005 .. KL_theta: 8846.33 .. KL_eta: 399.07 .. KL_alpha: 1388109.75 .. Rec_loss: 33216375.0 .. NELBO: 34613730.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->62 .. LR: 0.005 .. KL_theta: 7819.16 .. KL_eta: 437.94 .. KL_alpha: 1312318.5 .. Rec_loss: 33299475.0 .. NELBO: 34620050.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->63 .. LR: 0.005 .. KL_theta: 7674.22 .. KL_eta: 509.53 .. KL_alpha: 1331174.25 .. Rec_loss: 33419052.0 .. NELBO: 34758410.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->64 .. LR: 0.005 .. KL_theta: 8428.92 .. KL_eta: 404.33 .. KL_alpha: 1357256.62 .. Rec_loss: 32919661.0 .. NELBO: 34285750.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->65 .. LR: 0.005 .. KL_theta: 9802.11 .. KL_eta: 466.43 .. KL_alpha: 1324820.62 .. Rec_loss: 33094655.0 .. NELBO: 34429744.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->66 .. LR: 0.005 .. KL_theta: 12641.65 .. KL_eta: 351.08 .. KL_alpha: 1317402.56 .. Rec_loss: 33267074.0 .. NELBO: 34597472.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->67 .. LR: 0.005 .. KL_theta: 12988.47 .. KL_eta: 505.41 .. KL_alpha: 1329323.06 .. Rec_loss: 33095339.0 .. NELBO: 34438156.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->68 .. LR: 0.005 .. KL_theta: 12429.41 .. KL_eta: 504.65 .. KL_alpha: 1327674.69 .. Rec_loss: 33314474.0 .. NELBO: 34655084.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->69 .. LR: 0.005 .. KL_theta: 9876.86 .. KL_eta: 469.0 .. KL_alpha: 1303574.31 .. Rec_loss: 33094779.0 .. NELBO: 34408700.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->70 .. LR: 0.005 .. KL_theta: 5939.18 .. KL_eta: 631.44 .. KL_alpha: 1313224.31 .. Rec_loss: 33211498.0 .. NELBO: 34531294.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->71 .. LR: 0.005 .. KL_theta: 6010.31 .. KL_eta: 502.19 .. KL_alpha: 1276786.44 .. Rec_loss: 32936373.0 .. NELBO: 34219672.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->72 .. LR: 0.005 .. KL_theta: 11148.64 .. KL_eta: 451.15 .. KL_alpha: 1273295.06 .. Rec_loss: 32895604.0 .. NELBO: 34180498.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->73 .. LR: 0.005 .. KL_theta: 16732.6 .. KL_eta: 593.23 .. KL_alpha: 1273103.5 .. Rec_loss: 33133001.0 .. NELBO: 34423432.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->74 .. LR: 0.005 .. KL_theta: 19072.58 .. KL_eta: 663.79 .. KL_alpha: 1287572.38 .. Rec_loss: 32899596.0 .. NELBO: 34206906.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->75 .. LR: 0.005 .. KL_theta: 15643.12 .. KL_eta: 843.22 .. KL_alpha: 1244793.69 .. Rec_loss: 32899313.0 .. NELBO: 34160592.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->76 .. LR: 0.005 .. KL_theta: 12157.08 .. KL_eta: 884.21 .. KL_alpha: 1266539.56 .. Rec_loss: 33036578.0 .. NELBO: 34316160.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->77 .. LR: 0.005 .. KL_theta: 11577.1 .. KL_eta: 817.81 .. KL_alpha: 1241790.06 .. Rec_loss: 33098252.0 .. NELBO: 34352438.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->78 .. LR: 0.005 .. KL_theta: 10308.46 .. KL_eta: 613.65 .. KL_alpha: 1234494.19 .. Rec_loss: 32897509.0 .. NELBO: 34142924.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->79 .. LR: 0.005 .. KL_theta: 10598.56 .. KL_eta: 1451.87 .. KL_alpha: 1226687.81 .. Rec_loss: 32981914.0 .. NELBO: 34220652.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->80 .. LR: 0.005 .. KL_theta: 15952.73 .. KL_eta: 549.05 .. KL_alpha: 1244438.19 .. Rec_loss: 32902298.0 .. NELBO: 34163238.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->81 .. LR: 0.005 .. KL_theta: 25235.62 .. KL_eta: 477.97 .. KL_alpha: 1215825.31 .. Rec_loss: 32828282.0 .. NELBO: 34069822.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->82 .. LR: 0.005 .. KL_theta: 15028.37 .. KL_eta: 467.49 .. KL_alpha: 1217168.25 .. Rec_loss: 32902040.0 .. NELBO: 34134704.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->83 .. LR: 0.005 .. KL_theta: 10559.38 .. KL_eta: 760.73 .. KL_alpha: 1218227.06 .. Rec_loss: 33003337.0 .. NELBO: 34232882.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->84 .. LR: 0.005 .. KL_theta: 9543.98 .. KL_eta: 346.83 .. KL_alpha: 1238013.06 .. Rec_loss: 32982409.0 .. NELBO: 34230312.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->85 .. LR: 0.005 .. KL_theta: 10804.01 .. KL_eta: 480.23 .. KL_alpha: 1205417.44 .. Rec_loss: 32869192.0 .. NELBO: 34085894.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->86 .. LR: 0.005 .. KL_theta: 10433.22 .. KL_eta: 444.91 .. KL_alpha: 1209229.56 .. Rec_loss: 32795558.0 .. NELBO: 34015666.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->87 .. LR: 0.005 .. KL_theta: 12509.1 .. KL_eta: 955.44 .. KL_alpha: 1177628.88 .. Rec_loss: 33016531.0 .. NELBO: 34207627.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->88 .. LR: 0.005 .. KL_theta: 13150.37 .. KL_eta: 908.77 .. KL_alpha: 1171725.38 .. Rec_loss: 32968412.0 .. NELBO: 34154198.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->89 .. LR: 0.005 .. KL_theta: 13927.87 .. KL_eta: 1000.28 .. KL_alpha: 1164055.69 .. Rec_loss: 32855601.0 .. NELBO: 34034584.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->90 .. LR: 0.005 .. KL_theta: 13014.87 .. KL_eta: 1010.21 .. KL_alpha: 1143580.62 .. Rec_loss: 32847642.0 .. NELBO: 34005248.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->91 .. LR: 0.005 .. KL_theta: 14266.36 .. KL_eta: 760.81 .. KL_alpha: 1177666.81 .. Rec_loss: 32722378.0 .. NELBO: 33915071.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->92 .. LR: 0.005 .. KL_theta: 27336.34 .. KL_eta: 641.64 .. KL_alpha: 1155505.94 .. Rec_loss: 32714614.0 .. NELBO: 33898100.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->93 .. LR: 0.005 .. KL_theta: 15628.62 .. KL_eta: 715.38 .. KL_alpha: 1165081.56 .. Rec_loss: 32761663.0 .. NELBO: 33943089.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->94 .. LR: 0.005 .. KL_theta: 8610.17 .. KL_eta: 646.9 .. KL_alpha: 1141900.31 .. Rec_loss: 32865056.0 .. NELBO: 34016213.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->95 .. LR: 0.005 .. KL_theta: 11539.31 .. KL_eta: 468.63 .. KL_alpha: 1136277.81 .. Rec_loss: 32806185.0 .. NELBO: 33954472.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->96 .. LR: 0.005 .. KL_theta: 14704.37 .. KL_eta: 452.34 .. KL_alpha: 1134836.06 .. Rec_loss: 32861878.0 .. NELBO: 34011872.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->97 .. LR: 0.005 .. KL_theta: 18432.08 .. KL_eta: 472.8 .. KL_alpha: 1120413.06 .. Rec_loss: 32553848.0 .. NELBO: 33693166.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->98 .. LR: 0.005 .. KL_theta: 19104.55 .. KL_eta: 463.21 .. KL_alpha: 1104111.5 .. Rec_loss: 32247303.0 .. NELBO: 33370982.0\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->99 .. LR: 0.005 .. KL_theta: 22630.27 .. KL_eta: 482.87 .. KL_alpha: 1113617.31 .. Rec_loss: 32746901.0 .. NELBO: 33883632.0\n",
      "****************************************************************************************************\n",
      "saving topic matrix beta...\n"
     ]
    }
   ],
   "source": [
    "## train model on data by looping through multiple epochs\n",
    "best_epoch = 0\n",
    "best_val_ppl = 1e9\n",
    "all_val_ppls = []\n",
    "for epoch in range(1, args.epochs):\n",
    "    train(epoch)\n",
    "    # if epoch % args.visualize_every == 0:\n",
    "    #     visualize()\n",
    "    # val_ppl = get_completion_ppl('val')\n",
    "    # print('val_ppl: ', val_ppl)\n",
    "    # if val_ppl < best_val_ppl:\n",
    "    #     with open(ckpt, 'wb') as f:\n",
    "    #         torch.save(model, f)\n",
    "    #     best_epoch = epoch\n",
    "    #     best_val_ppl = val_ppl\n",
    "    # else:\n",
    "    ## check whether to anneal lr\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    if args.anneal_lr and (len(all_val_ppls) > args.nonmono and val_ppl > min(all_val_ppls[:-args.nonmono]) and lr > 1e-5):\n",
    "        optimizer.param_groups[0]['lr'] /= args.lr_factor\n",
    "    #all_val_ppls.append(val_ppl)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('saving topic matrix beta...')\n",
    "    alpha = model.mu_q_alpha\n",
    "    beta = model.get_beta(alpha).cpu().numpy()\n",
    "    scipy.io.savemat(ckpt+'_beta.mat', {'values': beta}, do_compression=True)\n",
    "    if args.train_embeddings:\n",
    "        print('saving word embedding matrix rho...')\n",
    "        rho = model.rho.weight.cpu().numpy()\n",
    "        scipy.io.savemat(ckpt+'_rho.mat', {'values': rho}, do_compression=True)\n",
    "    # print('computing validation perplexity...')\n",
    "    # val_ppl = get_completion_ppl('val')\n",
    "    # print('computing test perplexity...')\n",
    "    # test_ppl = get_completion_ppl('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1844c148-921d-44e8-a5db-d6dc8e49579b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5, 25297)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta.shape  # gives the allocation of each word in each time period to each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3acd5532-3301-4fb5-92d4-c4cf63bcfdc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000032"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta[0][0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "91cb32b9-1325-4a18-a9c2-6b7f495e2b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 96])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha.shape # gives the topic embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ceb5f26e-7c56-4f21-8cae-00a962567d07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5467, 0.119253635)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta[0][0].argmax(), beta[0][0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c38f509a-466c-44a3-b99c-b40639589224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "650eab18-c8fc-44bf-b247-c5a05d2f9618",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 25297/25297 [00:00<00:00, 31413.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23634"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = []\n",
    "for e in tqdm(embeddings):\n",
    "    dists.append(euclidean(e, alpha[0][0].detach().numpy()))\n",
    "\n",
    "np.argmin(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ff7e94ff-629a-49ca-b780-df360a861b91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta:  torch.Size([10, 5, 25297])\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Visualize topics...\n",
      "Topic 0 .. Time: 0 ===> ['setup', 'mainstream', 'instance', 'critic', 'powerless', 'smith', 'mc_id', 'outmoded', 'unconsolidated', 'realign', 'mark', 'kingston', 'geometrically', 'tonight', 'jobsone', 'systemthe', 'gainful', 'continuum', 'regard']\n",
      "Topic 0 .. Time: 2 ===> ['geological', 'alumnus', 'their', 'examplesthe', 'jobsthe', 'strongest', 'modelingwhen', 'perpetuity', 'congruence', 'ostry', 'unconnected', 'giacomo', 'fixes', 'facilitation', 'reputable', 'apt', 'translate', 'fade', 'waiting']\n",
      "Topic 1 .. Time: 0 ===> ['hft', 'conglomerate', 'ordinarily', 'underutilized', 'ownit', 'grandson', 'inscribe', 'toofrequent', 'betr', 'timesis', 'minnow', 'iron', 'expiration', 'database', 'nagging', 's', 'auditor', 'laidler', 'audible']\n",
      "Topic 1 .. Time: 2 ===> ['helicopter', 'penultimate', 'downhill', 'dyke', 'hft', 'again', 'williamette', 'swaption', 'then', 'shrinkage', 'bernanke', 'rude', 'granite', 'lagged', 'policymakingin', '1425585417', 'futurea', 'locate', 'clerc']\n",
      "Topic 2 .. Time: 0 ===> ['inflated', 'door', 'risksor', 'controversy', 'savvy', 'advise', 'tuition', 'catalyze', 'subord_debt_2000', 'globalization', 'imfpubs', 'inequi', 'panglossian', 'residents', 'squirm', 'allude', 'dsl', 'reservea', '643']\n",
      "Topic 2 .. Time: 2 ===> ['manufactured', 'denial', 'growthand', 'banksgather', 'moreno', 'werlang', 'agamus', 'realtytrac', '25th', 'changeover', 'weaknessalso', 'operationsthe', 'towel', 'unspoken', 'vasdev', 'maverick', 'reformatte', 'ssrn', 'onepredatory']\n",
      "Topic 3 .. Time: 0 ===> ['goodin', 'overkill', 'pillar', 'consulting', 'adopt', 'reaction', 'prioritize', 'bayliss', 'necessitate', 'supplywhether', 'disclosureespecially', 'imperfect', 'remainder', 'musician', 'adult', 'noncentral', 'depreciate', 'unquestione', 'withthe']\n",
      "Topic 3 .. Time: 2 ===> ['socialize', 'contingency', 'tailed', 'concentrate', 'allege', 'uncertaintygiven', 'compulsory', 'pinch', 'demandeven', 'located', 'coauthore', 'cyclicality', 'leaky', 'detachment', 'colloquially', 'compelling', 'exitin', 'clearer', 'marketsin']\n",
      "Topic 4 .. Time: 0 ===> ['moreno', 'policymaking', 'pantheon', 'availableto', 'resumption', 'youth', 'station', 'tripling', 'servicesprimarily', 'revenue', '20data', 'structures', 'accurate', 'criticized', 'disincentivize', 'alternating', 'manny', 'werlang', 'reinstatement']\n",
      "Topic 4 .. Time: 2 ===> ['funded', 'don', 'blake', 'socialize', 'termwere', 'disqualification', 'domanski', 'made', 'vastly', '1979', 'depressionor', 'sibyl', 'bresnahan', 'fruitsin', 'policymakers', 'muted', 'reemploye', 'betterreflect', 'loansappeare']\n",
      "Topic 5 .. Time: 0 ===> ['workforces', 'recital', 'wander', 'thwart', 'earliernamely', 'prepare', 'statue', 'helpfully', 'defensibility', 'nation', 'chapel', 'rose', 'collectionbank', 'deficient', 'bair', 'golden', 'debilitate', 'overshooting', 'plaine']\n",
      "Topic 5 .. Time: 2 ===> ['iron', 'toofrequent', 'cutler', 'mandate', 'oblivion', 'fascinate', 'globalizer', 'wayside', 'disappearance', 'worry', 'regularization', 'keenly', 'deus', 'comforting', 'sustainability', 'newcomerspurchase', 'rates', 'diploma', 'outputonly']\n",
      "Topic 6 .. Time: 0 ===> ['fragile', 'discouraging', 'logo', 'causeswhat', 'calledand', 'conclusionto', 'marder', 'hugh', 'cautiousness', 'pairing', 'fireplace', 'askance', 'afternoon', 'kitchen', 'tableidea', 'ounce', 'smsa', 'lukewarm', 'eugene']\n",
      "Topic 6 .. Time: 2 ===> ['agree', 'predictive', 'par', 'scalable', 'institutionsare', '1992', 'siteas', 'individualism', 'larger', 'obviouscapturing', 'collegiality', 'clearinghouses', 'aloneencourage', 'possession', 'dazzling', 'expedition', 'multifaceted', 'dealer', 'center']\n",
      "Topic 7 .. Time: 0 ===> ['microfinance', 'policymaking', 'sustainability', 'revenue', 'ranking', 'outputonly', 'samolyk', 'exceedingly', 'betr', 'coibion', 'controllable', 'harp', 'ihcs', 'dictum', 'availableto', 'telegraphic', 'auditor', 'pantheon', '014']\n",
      "Topic 7 .. Time: 2 ===> ['exuberanceabout', 'suggestionsinclude', 'counterargument', 'merchandising', 'game', 'exit', 'scope', 'dollar', 'bachelor', 'strandsthe', 'politely', 'informationboth', 'ltvs', 'elsasser', 'issuancei', 'fickle', 'damjan', 'racetrack', 'interconnectiveness']\n",
      "Topic 8 .. Time: 0 ===> ['regionalism', 'scapegoat', 'reprogramme', 'drill', 'unholy', 'fundation', 'careful', 'mfp', 'tear', 'housewife', 'outlookin', 'policymove', 'donation', 'moaning', 'decentralization', 'pensions', 'chatter', 'bankeri', 'handle']\n",
      "Topic 8 .. Time: 2 ===> ['raj', 'marketsemerge', 'ain', 'unaccountable', 'stubborn', 'projectionsbecause', 'formative', 'honestly', 'honey', 'kocherlakota', 'experiment5this', 'externalitiesthe', 'economyhave', 'unwarranted', 'percentuntil', 'jerseyan', 'repetitive', 'successto', 'dartmouth']\n",
      "Topic 9 .. Time: 0 ===> ['denominate', 'inevitability', 'movement', 'bio', 'ledgerthat', 'disability', 'managementfocusing', 'dive', 'pawel', 'yearscredit', 'clarifie', 'codification', 'danceand', 'governancea', 'financeundoubtedly', 'canada', 'savinginfluence', 'colorado', 'origination']\n",
      "Topic 9 .. Time: 2 ===> ['elaborate', 'strongest', 'real', 'consisting', 'dn', 'concordat', 'fabulous', 'disrupt', 'changesin', 'shapea', 'grandson', 'optometry', 'eea', 'eld', 'record', 'endure', 'reappoint', 'installer', 'sm']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    alpha = model.mu_q_alpha\n",
    "    beta = model.get_beta(alpha) \n",
    "    print('beta: ', beta.size())\n",
    "    print('\\n')\n",
    "    print('#'*100)\n",
    "    print('Visualize topics...')\n",
    "    times = [0, 2]\n",
    "    topics_words = []\n",
    "    for k in range(args.num_topics):\n",
    "        for t in times:\n",
    "            gamma = beta[k, t, :]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-args.num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words]\n",
    "            topics_words.append(' '.join(topic_words))\n",
    "            print('Topic {} .. Time: {} ===> {}'.format(k, t, topic_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3731cf4-ead1-496c-a683-48bcb2630ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
