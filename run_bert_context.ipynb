{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293077f9-46f1-4489-b8ac-b2de4bef194a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/amarvenu/miniconda3/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from src import data\n",
    "from src.detm import DETM\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from analysis_utils import get_detm_topics, topic_diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1309530f-89db-4946-b9ea-bce4cc36ea74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "680dc701-a07d-46a5-9cc4-d351bc25fef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### data and file related arguments\n",
    "arg_str = \"\"\"\n",
    "parser.add_argument('--dataset', type=str, default='un', help='name of corpus')\n",
    "parser.add_argument('--data_path', type=str, default='un/', help='directory containing data')\n",
    "parser.add_argument('--emb_path', type=str, default='skipgram/embeddings.txt', help='directory containing embeddings')\n",
    "parser.add_argument('--save_path', type=str, default='./results', help='path to save results')\n",
    "parser.add_argument('--batch_size', type=int, default=1000, help='number of documents in a batch for training')\n",
    "parser.add_argument('--min_df', type=int, default=100, help='to get the right data..minimum document frequency')\n",
    "\n",
    "### model-related arguments\n",
    "parser.add_argument('--num_topics', type=int, default=50, help='number of topics')\n",
    "parser.add_argument('--rho_size', type=int, default=300, help='dimension of rho')\n",
    "parser.add_argument('--emb_size', type=int, default=300, help='dimension of embeddings')\n",
    "parser.add_argument('--t_hidden_size', type=int, default=800, help='dimension of hidden space of q(theta)')\n",
    "parser.add_argument('--theta_act', type=str, default='relu', help='tanh, softplus, relu, rrelu, leakyrelu, elu, selu, glu)')\n",
    "parser.add_argument('--train_embeddings', type=int, default=1, help='whether to fix rho or train it')\n",
    "parser.add_argument('--eta_nlayers', type=int, default=3, help='number of layers for eta')\n",
    "parser.add_argument('--eta_hidden_size', type=int, default=200, help='number of hidden units for rnn')\n",
    "parser.add_argument('--delta', type=float, default=0.005, help='prior variance')\n",
    "\n",
    "### optimization-related arguments\n",
    "parser.add_argument('--lr', type=float, default=0.005, help='learning rate')\n",
    "parser.add_argument('--lr_factor', type=float, default=4.0, help='divide learning rate by this')\n",
    "parser.add_argument('--epochs', type=int, default=100, help='number of epochs to train')\n",
    "parser.add_argument('--mode', type=str, default='train', help='train or eval model')\n",
    "parser.add_argument('--optimizer', type=str, default='adam', help='choice of optimizer')\n",
    "parser.add_argument('--seed', type=int, default=2019, help='random seed (default: 1)')\n",
    "parser.add_argument('--enc_drop', type=float, default=0.0, help='dropout rate on encoder')\n",
    "parser.add_argument('--eta_dropout', type=float, default=0.0, help='dropout rate on rnn for eta')\n",
    "parser.add_argument('--clip', type=float, default=0.0, help='gradient clipping')\n",
    "parser.add_argument('--nonmono', type=int, default=10, help='number of bad hits allowed')\n",
    "parser.add_argument('--wdecay', type=float, default=1.2e-6, help='some l2 regularization')\n",
    "parser.add_argument('--anneal_lr', type=int, default=0, help='whether to anneal the learning rate or not')\n",
    "parser.add_argument('--bow_norm', type=int, default=1, help='normalize the bows or not')\n",
    "\n",
    "### evaluation, visualization, and logging-related arguments\n",
    "parser.add_argument('--num_words', type=int, default=20, help='number of words for topic viz')\n",
    "parser.add_argument('--log_interval', type=int, default=10, help='when to log training')\n",
    "parser.add_argument('--visualize_every', type=int, default=1, help='when to visualize results')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=1000, help='input batch size for evaluation')\n",
    "parser.add_argument('--load_from', type=str, default='', help='the name of the ckpt to eval from')\n",
    "parser.add_argument('--tc', type=int, default=0, help='whether to compute tc or not')\n",
    "\"\"\".split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c037c86-db04-4fa8-8fc5-13f99332813d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keys = [x.strip(\"parser.add_argument('\").split(',')[0].strip('--').strip(\"'\") for x in arg_str if (len(x) > 0) and (not x.startswith('#'))]\n",
    "values = [x.strip(\"parser.add_argument('\").split(',')[2].strip(\" default=\").strip(\"'\") for x in arg_str if (len(x) > 0) and (not x.startswith('#'))]\n",
    "tmp_dict = dict(zip(keys, values))\n",
    "\n",
    "for k, v in tmp_dict.items():\n",
    "    if v.isnumeric():\n",
    "        tmp_dict[k] = int(v)\n",
    "    elif ('.' in v) and (v[0].isnumeric()):\n",
    "        tmp_dict[k] = float(v)    \n",
    "\n",
    "args = AttrDict()\n",
    "args.update(tmp_dict)\n",
    "\n",
    "args.train_embeddings = 0\n",
    "args.rho_size = 768\n",
    "args.num_topics = 10\n",
    "args.batch_size = 100\n",
    "args.epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "574d5721-f212-49cf-afe9-dddf97ee08d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_arr = np.load('test_data_bert_context.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e20155d7-2fff-4d89-a127-ba89a66d01f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_tokens = train_arr['train_tokens']\n",
    "train_counts = train_arr['train_counts']\n",
    "train_times = train_arr['train_times']\n",
    "vocab = train_arr['vocab']\n",
    "embeddings = train_arr['embeddings']\n",
    "\n",
    "args.num_times = len(np.unique(train_times))\n",
    "args.num_docs_train = len(train_tokens)\n",
    "args.vocab_size = len(vocab)\n",
    "args.num_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21ada37c-a91d-4293-a147-7de92607ae8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0/2\n",
      "CPU times: user 16.6 s, sys: 82 ms, total: 16.7 s\n",
      "Wall time: 589 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_rnn_inp = data.get_rnn_input(train_tokens, train_counts, train_times, args.num_times, args.vocab_size, args.num_docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6b0df7f-8ffb-4dea-84c5-3faa403ac5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(args['save_path']):\n",
    "    os.makedirs(args['save_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24c7d9fa-102e-4d7e-8734-53096816d0a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if args.mode == 'eval':\n",
    "    ckpt = args.load_from\n",
    "else:\n",
    "    ckpt = os.path.join(args.save_path, \n",
    "        'bert_context_detm_{}_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_L_{}_minDF_{}_trainEmbeddings_{}'.format(\n",
    "        args.dataset, args.num_topics, args.t_hidden_size, args.optimizer, args.clip, args.theta_act, \n",
    "            args.lr, args.batch_size, args.rho_size, args.eta_nlayers, args.min_df, args.train_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "047e40c2-92aa-4707-a77c-3db9313831cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DETM(\n",
       "  (t_drop): Dropout(p=0.0, inplace=False)\n",
       "  (theta_act): ReLU()\n",
       "  (q_theta): Sequential(\n",
       "    (0): Linear(in_features=4948, out_features=800, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (mu_q_theta): Linear(in_features=800, out_features=10, bias=True)\n",
       "  (logsigma_q_theta): Linear(in_features=800, out_features=10, bias=True)\n",
       "  (q_eta_map): Linear(in_features=4938, out_features=200, bias=True)\n",
       "  (q_eta): LSTM(200, 200, num_layers=3)\n",
       "  (mu_q_eta): Linear(in_features=210, out_features=10, bias=True)\n",
       "  (logsigma_q_eta): Linear(in_features=210, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embeddings = torch.from_numpy(embeddings).to(device)\n",
    "args.embeddings_dim = embeddings.size()\n",
    "\n",
    "model = DETM(args, embeddings)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f54de551-16ca-419d-876e-1ba965d39d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if args.optimizer == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'adadelta':\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'asgd':\n",
    "    optimizer = optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\n",
    "else:\n",
    "    print('Defaulting to vanilla SGD')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a92f90-5e83-4bf4-a3a0-2d9390061275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    \"\"\"Train DETM on data for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    acc_loss = 0\n",
    "    acc_nll = 0\n",
    "    acc_kl_theta_loss = 0\n",
    "    acc_kl_eta_loss = 0\n",
    "    acc_kl_alpha_loss = 0\n",
    "    cnt = 0\n",
    "    indices = torch.randperm(args.num_docs_train)\n",
    "    indices = torch.split(indices, args.batch_size) \n",
    "    for idx, ind in enumerate(indices):\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        data_batch, times_batch = data.get_batch(\n",
    "            train_tokens, train_counts, ind, args.vocab_size, args.emb_size, temporal=True, times=train_times)\n",
    "        sums = data_batch.sum(1).unsqueeze(1)\n",
    "        if args.bow_norm:\n",
    "            normalized_data_batch = data_batch / sums\n",
    "        else:\n",
    "            normalized_data_batch = data_batch\n",
    "\n",
    "        loss, nll, kl_alpha, kl_eta, kl_theta = model(data_batch, normalized_data_batch, times_batch, train_rnn_inp, args.num_docs_train)\n",
    "        loss.backward()\n",
    "        if args.clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        acc_loss += torch.sum(loss).item()\n",
    "        acc_nll += torch.sum(nll).item()\n",
    "        acc_kl_theta_loss += torch.sum(kl_theta).item()\n",
    "        acc_kl_eta_loss += torch.sum(kl_eta).item()\n",
    "        acc_kl_alpha_loss += torch.sum(kl_alpha).item()\n",
    "        cnt += 1\n",
    "\n",
    "        if idx % args.log_interval == 0 and idx > 0:\n",
    "            cur_loss = round(acc_loss / cnt, 2) \n",
    "            cur_nll = round(acc_nll / cnt, 2) \n",
    "            cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "            cur_kl_eta = round(acc_kl_eta_loss / cnt, 2) \n",
    "            cur_kl_alpha = round(acc_kl_alpha_loss / cnt, 2) \n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            print('Epoch: {} .. batch: {}/{} .. LR: {} .. KL_theta: {} .. KL_eta: {} .. KL_alpha: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "                epoch, idx, len(indices), lr, cur_kl_theta, cur_kl_eta, cur_kl_alpha, cur_nll, cur_loss))\n",
    "    \n",
    "    cur_loss = round(acc_loss / cnt, 2) \n",
    "    cur_nll = round(acc_nll / cnt, 2) \n",
    "    cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "    cur_kl_eta = round(acc_kl_eta_loss / cnt, 2) \n",
    "    cur_kl_alpha = round(acc_kl_alpha_loss / cnt, 2) \n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    print('*'*100)\n",
    "    print('Epoch----->{} .. LR: {} .. KL_theta: {} .. KL_eta: {} .. KL_alpha: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "            epoch, lr, cur_kl_theta, cur_kl_eta, cur_kl_alpha, cur_nll, cur_loss))\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b51ea22c-39c7-4ece-a26c-2c97c817146f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 3638.13 .. KL_eta: 2042.36 .. KL_alpha: 11925041.73 .. Rec_loss: 31285508.18 .. NELBO: 43216229.82\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 2660.83 .. KL_eta: 1340.28 .. KL_alpha: 11717719.12 .. Rec_loss: 31736794.0 .. NELBO: 43458513.41\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 832.45 .. KL_eta: 53.43 .. KL_alpha: 10888620.0 .. Rec_loss: 30978662.55 .. NELBO: 41868167.64\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 1022.51 .. KL_eta: 57.75 .. KL_alpha: 10716609.94 .. Rec_loss: 31798209.06 .. NELBO: 42515898.82\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 735.16 .. KL_eta: 58.54 .. KL_alpha: 9968854.36 .. Rec_loss: 31401424.91 .. NELBO: 41371072.73\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 801.44 .. KL_eta: 58.33 .. KL_alpha: 9831642.47 .. Rec_loss: 31786018.12 .. NELBO: 41618520.47\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 666.23 .. KL_eta: 54.28 .. KL_alpha: 9228451.27 .. Rec_loss: 31943873.45 .. NELBO: 41173045.45\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 642.19 .. KL_eta: 54.88 .. KL_alpha: 9089101.65 .. Rec_loss: 31728934.47 .. NELBO: 40818732.94\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 1295.74 .. KL_eta: 59.82 .. KL_alpha: 8564062.45 .. Rec_loss: 32067300.0 .. NELBO: 40632718.18\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 1229.07 .. KL_eta: 58.25 .. KL_alpha: 8454392.82 .. Rec_loss: 31564939.41 .. NELBO: 40020619.29\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 813.82 .. KL_eta: 52.26 .. KL_alpha: 7970477.18 .. Rec_loss: 31689917.64 .. NELBO: 39661260.0\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 1291.8 .. KL_eta: 55.01 .. KL_alpha: 7889996.35 .. Rec_loss: 31666656.94 .. NELBO: 39557999.76\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 965.13 .. KL_eta: 52.24 .. KL_alpha: 7493747.41 .. Rec_loss: 31938175.09 .. NELBO: 39432939.27\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 1091.67 .. KL_eta: 52.29 .. KL_alpha: 7430815.21 .. Rec_loss: 31705875.06 .. NELBO: 39137833.41\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 1787.61 .. KL_eta: 71.13 .. KL_alpha: 7120748.14 .. Rec_loss: 31955997.64 .. NELBO: 39078605.09\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 2166.69 .. KL_eta: 73.99 .. KL_alpha: 7040265.44 .. Rec_loss: 31648153.29 .. NELBO: 38690659.53\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 1144.88 .. KL_eta: 75.14 .. KL_alpha: 6752028.14 .. Rec_loss: 31783482.18 .. NELBO: 38536730.18\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 1175.9 .. KL_eta: 75.16 .. KL_alpha: 6703746.91 .. Rec_loss: 31565087.06 .. NELBO: 38270084.71\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 1137.49 .. KL_eta: 63.1 .. KL_alpha: 6439161.14 .. Rec_loss: 31652662.55 .. NELBO: 38093024.0\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 1120.39 .. KL_eta: 72.31 .. KL_alpha: 6399864.97 .. Rec_loss: 31645708.59 .. NELBO: 38046766.35\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 1835.83 .. KL_eta: 84.67 .. KL_alpha: 6169778.73 .. Rec_loss: 31540592.91 .. NELBO: 37712292.0\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 1623.87 .. KL_eta: 76.65 .. KL_alpha: 6128636.47 .. Rec_loss: 31725002.94 .. NELBO: 37855339.76\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 2278.0 .. KL_eta: 66.18 .. KL_alpha: 5922517.41 .. Rec_loss: 31911913.27 .. NELBO: 37836774.55\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 2552.9 .. KL_eta: 85.32 .. KL_alpha: 5880295.09 .. Rec_loss: 31698657.29 .. NELBO: 37581591.06\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 2168.83 .. KL_eta: 159.21 .. KL_alpha: 5717218.32 .. Rec_loss: 31306655.27 .. NELBO: 37026201.09\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 2177.85 .. KL_eta: 131.76 .. KL_alpha: 5688178.44 .. Rec_loss: 31756541.06 .. NELBO: 37447028.71\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 2090.17 .. KL_eta: 84.33 .. KL_alpha: 5518884.14 .. Rec_loss: 31883984.73 .. NELBO: 37405043.27\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 1790.92 .. KL_eta: 115.71 .. KL_alpha: 5476782.32 .. Rec_loss: 31685296.12 .. NELBO: 37163985.88\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 1261.86 .. KL_eta: 110.32 .. KL_alpha: 5330882.95 .. Rec_loss: 31191914.18 .. NELBO: 36524168.91\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 1519.08 .. KL_eta: 95.52 .. KL_alpha: 5294979.29 .. Rec_loss: 31792314.94 .. NELBO: 37088908.12\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 1800.97 .. KL_eta: 76.81 .. KL_alpha: 5191707.18 .. Rec_loss: 31909678.55 .. NELBO: 37103263.64\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 1763.12 .. KL_eta: 75.2 .. KL_alpha: 5155077.62 .. Rec_loss: 31790363.41 .. NELBO: 36947279.53\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 2532.89 .. KL_eta: 80.65 .. KL_alpha: 5041873.23 .. Rec_loss: 30901844.55 .. NELBO: 35946331.27\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 2367.66 .. KL_eta: 74.32 .. KL_alpha: 5015479.91 .. Rec_loss: 31654237.18 .. NELBO: 36672158.94\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 2757.16 .. KL_eta: 80.02 .. KL_alpha: 4908105.23 .. Rec_loss: 31477655.27 .. NELBO: 36388597.45\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 3058.22 .. KL_eta: 78.53 .. KL_alpha: 4894515.5 .. Rec_loss: 31747554.47 .. NELBO: 36645206.12\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 2542.55 .. KL_eta: 80.28 .. KL_alpha: 4803574.45 .. Rec_loss: 31886022.55 .. NELBO: 36692220.36\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 2280.54 .. KL_eta: 89.64 .. KL_alpha: 4798502.97 .. Rec_loss: 31811578.24 .. NELBO: 36612451.53\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 1435.36 .. KL_eta: 127.59 .. KL_alpha: 4723931.5 .. Rec_loss: 31818590.18 .. NELBO: 36544085.09\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 1717.05 .. KL_eta: 123.55 .. KL_alpha: 4705515.29 .. Rec_loss: 31719725.18 .. NELBO: 36427081.41\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 959.99 .. KL_eta: 116.92 .. KL_alpha: 4634262.05 .. Rec_loss: 31660898.91 .. NELBO: 36296237.82\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 935.15 .. KL_eta: 106.92 .. KL_alpha: 4623248.47 .. Rec_loss: 31946987.65 .. NELBO: 36571278.59\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 901.01 .. KL_eta: 69.95 .. KL_alpha: 4562719.09 .. Rec_loss: 31659692.18 .. NELBO: 36223382.91\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 949.83 .. KL_eta: 68.68 .. KL_alpha: 4546086.03 .. Rec_loss: 31732561.06 .. NELBO: 36279666.35\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 931.41 .. KL_eta: 69.1 .. KL_alpha: 4482357.91 .. Rec_loss: 30916291.64 .. NELBO: 35399650.55\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 1009.79 .. KL_eta: 68.52 .. KL_alpha: 4473870.5 .. Rec_loss: 31771129.18 .. NELBO: 36246078.12\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 1308.97 .. KL_eta: 83.12 .. KL_alpha: 4418675.91 .. Rec_loss: 32391305.27 .. NELBO: 36811373.45\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 1203.98 .. KL_eta: 77.85 .. KL_alpha: 4405873.79 .. Rec_loss: 31711267.76 .. NELBO: 36118423.41\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 965.59 .. KL_eta: 68.13 .. KL_alpha: 4356871.27 .. Rec_loss: 31806380.36 .. NELBO: 36164284.73\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 968.86 .. KL_eta: 67.0 .. KL_alpha: 4344375.15 .. Rec_loss: 31830551.88 .. NELBO: 36175962.35\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 1075.03 .. KL_eta: 67.16 .. KL_alpha: 4312917.0 .. Rec_loss: 31799394.36 .. NELBO: 36113453.82\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 1021.27 .. KL_eta: 68.36 .. KL_alpha: 4294921.62 .. Rec_loss: 31858747.41 .. NELBO: 36154758.82\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 1015.65 .. KL_eta: 65.2 .. KL_alpha: 4235041.77 .. Rec_loss: 31748035.82 .. NELBO: 35984158.73\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 1026.65 .. KL_eta: 64.98 .. KL_alpha: 4226972.29 .. Rec_loss: 31828074.94 .. NELBO: 36056138.82\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 825.82 .. KL_eta: 62.34 .. KL_alpha: 4183346.16 .. Rec_loss: 31598230.91 .. NELBO: 35782465.27\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 896.92 .. KL_eta: 63.72 .. KL_alpha: 4179070.24 .. Rec_loss: 31865178.82 .. NELBO: 36045209.53\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 995.88 .. KL_eta: 65.28 .. KL_alpha: 4123639.61 .. Rec_loss: 31771520.73 .. NELBO: 35896221.09\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 980.8 .. KL_eta: 65.21 .. KL_alpha: 4117240.44 .. Rec_loss: 31802471.76 .. NELBO: 35920758.12\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 1034.11 .. KL_eta: 62.68 .. KL_alpha: 4089201.48 .. Rec_loss: 31715391.45 .. NELBO: 35805690.18\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 1243.01 .. KL_eta: 61.13 .. KL_alpha: 4077575.57 .. Rec_loss: 31799653.29 .. NELBO: 35878533.18\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 954.58 .. KL_eta: 63.76 .. KL_alpha: 4019673.11 .. Rec_loss: 32040734.36 .. NELBO: 36061425.64\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 1025.54 .. KL_eta: 65.25 .. KL_alpha: 4018823.51 .. Rec_loss: 31715190.71 .. NELBO: 35735105.41\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 877.92 .. KL_eta: 67.24 .. KL_alpha: 4002237.32 .. Rec_loss: 32041798.55 .. NELBO: 36044980.18\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 811.21 .. KL_eta: 66.82 .. KL_alpha: 3983931.82 .. Rec_loss: 31850721.53 .. NELBO: 35835530.47\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 642.56 .. KL_eta: 64.95 .. KL_alpha: 3948323.18 .. Rec_loss: 31860150.0 .. NELBO: 35809181.09\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 701.05 .. KL_eta: 64.23 .. KL_alpha: 3939739.69 .. Rec_loss: 31805835.88 .. NELBO: 35746340.47\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 3320.09 .. KL_eta: 156.17 .. KL_alpha: 3907042.48 .. Rec_loss: 31949097.82 .. NELBO: 35859617.09\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 3680.44 .. KL_eta: 293.31 .. KL_alpha: 3903007.74 .. Rec_loss: 31665629.76 .. NELBO: 35572611.65\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 1711.88 .. KL_eta: 523.51 .. KL_alpha: 3882354.0 .. Rec_loss: 31214472.0 .. NELBO: 35099060.73\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 1390.92 .. KL_eta: 419.68 .. KL_alpha: 3874079.16 .. Rec_loss: 31893808.0 .. NELBO: 35769697.41\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 782.51 .. KL_eta: 128.22 .. KL_alpha: 3837019.07 .. Rec_loss: 31422917.09 .. NELBO: 35260846.18\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 1005.17 .. KL_eta: 134.82 .. KL_alpha: 3837559.16 .. Rec_loss: 31857158.94 .. NELBO: 35695857.76\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 1119.44 .. KL_eta: 146.05 .. KL_alpha: 3820022.14 .. Rec_loss: 31863034.18 .. NELBO: 35684320.91\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 1125.59 .. KL_eta: 130.9 .. KL_alpha: 3812895.54 .. Rec_loss: 31753983.41 .. NELBO: 35568134.94\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 691.57 .. KL_eta: 79.77 .. KL_alpha: 3781116.43 .. Rec_loss: 31770836.91 .. NELBO: 35552724.18\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 659.56 .. KL_eta: 83.17 .. KL_alpha: 3778882.76 .. Rec_loss: 31857040.71 .. NELBO: 35636666.0\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 428.59 .. KL_eta: 101.11 .. KL_alpha: 3748526.98 .. Rec_loss: 31860323.45 .. NELBO: 35609379.45\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 521.36 .. KL_eta: 103.09 .. KL_alpha: 3744531.72 .. Rec_loss: 31817406.47 .. NELBO: 35562562.0\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 335.96 .. KL_eta: 90.12 .. KL_alpha: 3731061.8 .. Rec_loss: 32023418.91 .. NELBO: 35754906.91\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 302.03 .. KL_eta: 90.25 .. KL_alpha: 3724558.28 .. Rec_loss: 31862024.94 .. NELBO: 35586975.76\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 719.65 .. KL_eta: 94.45 .. KL_alpha: 3701093.14 .. Rec_loss: 32177155.27 .. NELBO: 35879062.18\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 729.57 .. KL_eta: 91.93 .. KL_alpha: 3689946.1 .. Rec_loss: 31729518.35 .. NELBO: 35420285.65\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 2305.82 .. KL_eta: 92.61 .. KL_alpha: 3681708.32 .. Rec_loss: 31831533.64 .. NELBO: 35515640.0\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 1838.25 .. KL_eta: 95.4 .. KL_alpha: 3681613.68 .. Rec_loss: 31758378.12 .. NELBO: 35441925.29\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 971.51 .. KL_eta: 119.42 .. KL_alpha: 3637667.98 .. Rec_loss: 32494816.36 .. NELBO: 36133574.55\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 812.29 .. KL_eta: 119.47 .. KL_alpha: 3638898.69 .. Rec_loss: 31824801.06 .. NELBO: 35464631.06\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 833.82 .. KL_eta: 119.15 .. KL_alpha: 3612779.77 .. Rec_loss: 31537916.55 .. NELBO: 35151649.64\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 2156.6 .. KL_eta: 135.03 .. KL_alpha: 3605445.37 .. Rec_loss: 31824781.41 .. NELBO: 35432518.12\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 1662.45 .. KL_eta: 211.06 .. KL_alpha: 3568442.89 .. Rec_loss: 32145861.09 .. NELBO: 35716177.82\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 1269.7 .. KL_eta: 194.42 .. KL_alpha: 3575333.16 .. Rec_loss: 31662117.41 .. NELBO: 35238915.06\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 316.65 .. KL_eta: 114.7 .. KL_alpha: 3578286.5 .. Rec_loss: 31908918.36 .. NELBO: 35487637.09\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 251.24 .. KL_eta: 103.5 .. KL_alpha: 3572460.69 .. Rec_loss: 31848590.47 .. NELBO: 35421406.12\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 252.96 .. KL_eta: 72.99 .. KL_alpha: 3554519.14 .. Rec_loss: 32110278.73 .. NELBO: 35665124.0\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 261.35 .. KL_eta: 69.66 .. KL_alpha: 3548642.15 .. Rec_loss: 31854973.76 .. NELBO: 35403947.29\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 448.92 .. KL_eta: 65.71 .. KL_alpha: 3527282.82 .. Rec_loss: 31095880.36 .. NELBO: 34623677.45\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 396.08 .. KL_eta: 66.88 .. KL_alpha: 3522604.9 .. Rec_loss: 32015825.29 .. NELBO: 35538893.18\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 206.83 .. KL_eta: 64.44 .. KL_alpha: 3494233.59 .. Rec_loss: 32073151.82 .. NELBO: 35567656.18\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 432.42 .. KL_eta: 63.72 .. KL_alpha: 3493706.06 .. Rec_loss: 31977808.71 .. NELBO: 35472010.35\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 1037.73 .. KL_eta: 64.68 .. KL_alpha: 3478795.93 .. Rec_loss: 31573742.36 .. NELBO: 35053640.91\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 861.73 .. KL_eta: 62.26 .. KL_alpha: 3473276.72 .. Rec_loss: 31773812.71 .. NELBO: 35248013.41\n",
      "****************************************************************************************************\n",
      "Epoch: 51 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 504.2 .. KL_eta: 59.42 .. KL_alpha: 3458261.86 .. Rec_loss: 31827972.55 .. NELBO: 35286797.09\n",
      "****************************************************************************************************\n",
      "Epoch----->51 .. LR: 0.005 .. KL_theta: 482.63 .. KL_eta: 59.0 .. KL_alpha: 3454901.38 .. Rec_loss: 31964967.53 .. NELBO: 35420410.0\n",
      "****************************************************************************************************\n",
      "Epoch: 52 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 1189.45 .. KL_eta: 77.32 .. KL_alpha: 3439725.39 .. Rec_loss: 32090642.18 .. NELBO: 35531632.91\n",
      "****************************************************************************************************\n",
      "Epoch----->52 .. LR: 0.005 .. KL_theta: 1082.55 .. KL_eta: 79.55 .. KL_alpha: 3429114.99 .. Rec_loss: 31827753.76 .. NELBO: 35258029.76\n",
      "****************************************************************************************************\n",
      "Epoch: 53 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 489.53 .. KL_eta: 78.03 .. KL_alpha: 3401004.32 .. Rec_loss: 32183717.45 .. NELBO: 35585289.45\n",
      "****************************************************************************************************\n",
      "Epoch----->53 .. LR: 0.005 .. KL_theta: 431.45 .. KL_eta: 75.31 .. KL_alpha: 3402539.18 .. Rec_loss: 31788897.76 .. NELBO: 35191943.76\n",
      "****************************************************************************************************\n",
      "Epoch: 54 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 155.4 .. KL_eta: 68.46 .. KL_alpha: 3397767.25 .. Rec_loss: 31989149.27 .. NELBO: 35387141.09\n",
      "****************************************************************************************************\n",
      "Epoch----->54 .. LR: 0.005 .. KL_theta: 173.86 .. KL_eta: 70.15 .. KL_alpha: 3392693.47 .. Rec_loss: 31859176.12 .. NELBO: 35252114.12\n",
      "****************************************************************************************************\n",
      "Epoch: 55 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 296.69 .. KL_eta: 66.98 .. KL_alpha: 3370250.34 .. Rec_loss: 31981840.36 .. NELBO: 35352455.27\n",
      "****************************************************************************************************\n",
      "Epoch----->55 .. LR: 0.005 .. KL_theta: 376.26 .. KL_eta: 67.91 .. KL_alpha: 3375340.34 .. Rec_loss: 31743592.82 .. NELBO: 35119378.0\n",
      "****************************************************************************************************\n",
      "Epoch: 56 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 387.87 .. KL_eta: 65.4 .. KL_alpha: 3357385.89 .. Rec_loss: 32193740.73 .. NELBO: 35551579.45\n",
      "****************************************************************************************************\n",
      "Epoch----->56 .. LR: 0.005 .. KL_theta: 440.56 .. KL_eta: 65.8 .. KL_alpha: 3353687.04 .. Rec_loss: 31757754.82 .. NELBO: 35111948.24\n",
      "****************************************************************************************************\n",
      "Epoch: 57 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 282.14 .. KL_eta: 63.53 .. KL_alpha: 3336304.57 .. Rec_loss: 31966121.64 .. NELBO: 35302772.18\n",
      "****************************************************************************************************\n",
      "Epoch----->57 .. LR: 0.005 .. KL_theta: 243.44 .. KL_eta: 63.07 .. KL_alpha: 3337984.32 .. Rec_loss: 31841838.82 .. NELBO: 35180130.12\n",
      "****************************************************************************************************\n",
      "Epoch: 58 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 333.64 .. KL_eta: 64.88 .. KL_alpha: 3314961.52 .. Rec_loss: 31956540.18 .. NELBO: 35271900.18\n",
      "****************************************************************************************************\n",
      "Epoch----->58 .. LR: 0.005 .. KL_theta: 534.35 .. KL_eta: 66.53 .. KL_alpha: 3313754.94 .. Rec_loss: 31773747.41 .. NELBO: 35088103.18\n",
      "****************************************************************************************************\n",
      "Epoch: 59 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 370.75 .. KL_eta: 70.52 .. KL_alpha: 3290302.93 .. Rec_loss: 32100659.45 .. NELBO: 35391403.64\n",
      "****************************************************************************************************\n",
      "Epoch----->59 .. LR: 0.005 .. KL_theta: 307.07 .. KL_eta: 68.91 .. KL_alpha: 3293054.99 .. Rec_loss: 31859450.35 .. NELBO: 35152881.53\n",
      "****************************************************************************************************\n",
      "Epoch: 60 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 164.93 .. KL_eta: 61.16 .. KL_alpha: 3286267.14 .. Rec_loss: 31677604.55 .. NELBO: 34964097.45\n",
      "****************************************************************************************************\n",
      "Epoch----->60 .. LR: 0.005 .. KL_theta: 215.57 .. KL_eta: 61.36 .. KL_alpha: 3283107.21 .. Rec_loss: 31774587.18 .. NELBO: 35057971.53\n",
      "****************************************************************************************************\n",
      "Epoch: 61 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 261.32 .. KL_eta: 57.45 .. KL_alpha: 3268073.7 .. Rec_loss: 31552277.45 .. NELBO: 34820670.36\n",
      "****************************************************************************************************\n",
      "Epoch----->61 .. LR: 0.005 .. KL_theta: 248.19 .. KL_eta: 57.86 .. KL_alpha: 3262469.51 .. Rec_loss: 31826035.18 .. NELBO: 35088810.47\n",
      "****************************************************************************************************\n",
      "Epoch: 62 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 165.54 .. KL_eta: 57.07 .. KL_alpha: 3264058.48 .. Rec_loss: 31514404.73 .. NELBO: 34778686.91\n",
      "****************************************************************************************************\n",
      "Epoch----->62 .. LR: 0.005 .. KL_theta: 146.13 .. KL_eta: 56.89 .. KL_alpha: 3253804.31 .. Rec_loss: 31877967.53 .. NELBO: 35131975.53\n",
      "****************************************************************************************************\n",
      "Epoch: 63 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 205.88 .. KL_eta: 61.97 .. KL_alpha: 3221017.25 .. Rec_loss: 31761148.36 .. NELBO: 34982433.27\n",
      "****************************************************************************************************\n",
      "Epoch----->63 .. LR: 0.005 .. KL_theta: 225.31 .. KL_eta: 62.91 .. KL_alpha: 3223745.88 .. Rec_loss: 31995107.18 .. NELBO: 35219140.94\n",
      "****************************************************************************************************\n",
      "Epoch: 64 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 383.56 .. KL_eta: 68.75 .. KL_alpha: 3229046.95 .. Rec_loss: 31326932.36 .. NELBO: 34556432.36\n",
      "****************************************************************************************************\n",
      "Epoch----->64 .. LR: 0.005 .. KL_theta: 392.51 .. KL_eta: 68.72 .. KL_alpha: 3221141.84 .. Rec_loss: 31789403.88 .. NELBO: 35011007.53\n",
      "****************************************************************************************************\n",
      "Epoch: 65 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 396.47 .. KL_eta: 66.44 .. KL_alpha: 3215943.02 .. Rec_loss: 31527183.64 .. NELBO: 34743590.18\n",
      "****************************************************************************************************\n",
      "Epoch----->65 .. LR: 0.005 .. KL_theta: 403.81 .. KL_eta: 64.21 .. KL_alpha: 3213823.15 .. Rec_loss: 31868046.0 .. NELBO: 35082337.76\n",
      "****************************************************************************************************\n",
      "Epoch: 66 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 264.38 .. KL_eta: 58.14 .. KL_alpha: 3186474.73 .. Rec_loss: 31379374.0 .. NELBO: 34566171.27\n",
      "****************************************************************************************************\n",
      "Epoch----->66 .. LR: 0.005 .. KL_theta: 223.52 .. KL_eta: 58.92 .. KL_alpha: 3185582.31 .. Rec_loss: 31774209.06 .. NELBO: 34960074.0\n",
      "****************************************************************************************************\n",
      "Epoch: 67 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 112.66 .. KL_eta: 60.64 .. KL_alpha: 3181212.75 .. Rec_loss: 32114710.36 .. NELBO: 35296097.27\n",
      "****************************************************************************************************\n",
      "Epoch----->67 .. LR: 0.005 .. KL_theta: 143.59 .. KL_eta: 60.19 .. KL_alpha: 3176644.94 .. Rec_loss: 31813296.47 .. NELBO: 34990145.53\n",
      "****************************************************************************************************\n",
      "Epoch: 68 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 177.78 .. KL_eta: 58.31 .. KL_alpha: 3146379.2 .. Rec_loss: 31754465.64 .. NELBO: 34901081.64\n",
      "****************************************************************************************************\n",
      "Epoch----->68 .. LR: 0.005 .. KL_theta: 159.39 .. KL_eta: 57.87 .. KL_alpha: 3152226.81 .. Rec_loss: 31842236.71 .. NELBO: 34994681.29\n",
      "****************************************************************************************************\n",
      "Epoch: 69 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 75.63 .. KL_eta: 58.22 .. KL_alpha: 3135345.82 .. Rec_loss: 31100293.09 .. NELBO: 34235771.82\n",
      "****************************************************************************************************\n",
      "Epoch----->69 .. LR: 0.005 .. KL_theta: 91.72 .. KL_eta: 58.26 .. KL_alpha: 3140732.24 .. Rec_loss: 31797378.94 .. NELBO: 34938260.59\n",
      "****************************************************************************************************\n",
      "Epoch: 70 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 189.34 .. KL_eta: 59.59 .. KL_alpha: 3134498.82 .. Rec_loss: 31822119.82 .. NELBO: 34956868.36\n",
      "****************************************************************************************************\n",
      "Epoch----->70 .. LR: 0.005 .. KL_theta: 180.68 .. KL_eta: 60.83 .. KL_alpha: 3125600.31 .. Rec_loss: 31805315.53 .. NELBO: 34931157.76\n",
      "****************************************************************************************************\n",
      "Epoch: 71 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 134.31 .. KL_eta: 66.31 .. KL_alpha: 3102580.27 .. Rec_loss: 32117009.09 .. NELBO: 35219790.36\n",
      "****************************************************************************************************\n",
      "Epoch----->71 .. LR: 0.005 .. KL_theta: 134.99 .. KL_eta: 66.36 .. KL_alpha: 3105951.07 .. Rec_loss: 31900655.88 .. NELBO: 35006808.71\n",
      "****************************************************************************************************\n",
      "Epoch: 72 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 312.24 .. KL_eta: 78.34 .. KL_alpha: 3102489.89 .. Rec_loss: 32189616.0 .. NELBO: 35292496.55\n",
      "****************************************************************************************************\n",
      "Epoch----->72 .. LR: 0.005 .. KL_theta: 310.79 .. KL_eta: 86.63 .. KL_alpha: 3100740.18 .. Rec_loss: 31740210.35 .. NELBO: 34841347.88\n",
      "****************************************************************************************************\n",
      "Epoch: 73 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 181.95 .. KL_eta: 73.68 .. KL_alpha: 3079730.98 .. Rec_loss: 31670252.55 .. NELBO: 34750240.36\n",
      "****************************************************************************************************\n",
      "Epoch----->73 .. LR: 0.005 .. KL_theta: 157.64 .. KL_eta: 70.38 .. KL_alpha: 3080097.79 .. Rec_loss: 31932648.82 .. NELBO: 35012975.41\n",
      "****************************************************************************************************\n",
      "Epoch: 74 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 294.78 .. KL_eta: 61.22 .. KL_alpha: 3085502.91 .. Rec_loss: 31525363.45 .. NELBO: 34611222.73\n",
      "****************************************************************************************************\n",
      "Epoch----->74 .. LR: 0.005 .. KL_theta: 283.33 .. KL_eta: 60.45 .. KL_alpha: 3082179.87 .. Rec_loss: 31806429.29 .. NELBO: 34888953.06\n",
      "****************************************************************************************************\n",
      "Epoch: 75 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 500.35 .. KL_eta: 69.85 .. KL_alpha: 3073593.5 .. Rec_loss: 31714720.0 .. NELBO: 34788882.55\n",
      "****************************************************************************************************\n",
      "Epoch----->75 .. LR: 0.005 .. KL_theta: 483.44 .. KL_eta: 69.59 .. KL_alpha: 3076599.85 .. Rec_loss: 31891262.0 .. NELBO: 34968413.76\n",
      "****************************************************************************************************\n",
      "Epoch: 76 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 249.69 .. KL_eta: 61.54 .. KL_alpha: 3050736.16 .. Rec_loss: 31540758.55 .. NELBO: 34591805.64\n",
      "****************************************************************************************************\n",
      "Epoch----->76 .. LR: 0.005 .. KL_theta: 247.1 .. KL_eta: 63.03 .. KL_alpha: 3046764.29 .. Rec_loss: 31896957.76 .. NELBO: 34944032.35\n",
      "****************************************************************************************************\n",
      "Epoch: 77 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 260.13 .. KL_eta: 63.93 .. KL_alpha: 3046268.25 .. Rec_loss: 31482720.73 .. NELBO: 34529312.91\n",
      "****************************************************************************************************\n",
      "Epoch----->77 .. LR: 0.005 .. KL_theta: 231.23 .. KL_eta: 62.38 .. KL_alpha: 3041701.53 .. Rec_loss: 31847538.82 .. NELBO: 34889534.59\n",
      "****************************************************************************************************\n",
      "Epoch: 78 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 81.79 .. KL_eta: 56.46 .. KL_alpha: 3025774.41 .. Rec_loss: 32015493.45 .. NELBO: 35041406.91\n",
      "****************************************************************************************************\n",
      "Epoch----->78 .. LR: 0.005 .. KL_theta: 73.48 .. KL_eta: 56.12 .. KL_alpha: 3020203.99 .. Rec_loss: 31814243.06 .. NELBO: 34834577.29\n",
      "****************************************************************************************************\n",
      "Epoch: 79 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 49.47 .. KL_eta: 55.36 .. KL_alpha: 3011286.3 .. Rec_loss: 32565427.82 .. NELBO: 35576817.82\n",
      "****************************************************************************************************\n",
      "Epoch----->79 .. LR: 0.005 .. KL_theta: 60.73 .. KL_eta: 56.17 .. KL_alpha: 3005864.96 .. Rec_loss: 31780735.88 .. NELBO: 34786716.82\n",
      "****************************************************************************************************\n",
      "Epoch: 80 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 47.91 .. KL_eta: 55.21 .. KL_alpha: 2988768.25 .. Rec_loss: 31587386.36 .. NELBO: 34576258.36\n",
      "****************************************************************************************************\n",
      "Epoch----->80 .. LR: 0.005 .. KL_theta: 54.28 .. KL_eta: 56.08 .. KL_alpha: 2989185.35 .. Rec_loss: 31863163.53 .. NELBO: 34852459.65\n",
      "****************************************************************************************************\n",
      "Epoch: 81 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 271.84 .. KL_eta: 55.18 .. KL_alpha: 2982628.73 .. Rec_loss: 31803117.27 .. NELBO: 34786073.09\n",
      "****************************************************************************************************\n",
      "Epoch----->81 .. LR: 0.005 .. KL_theta: 267.17 .. KL_eta: 54.41 .. KL_alpha: 2983943.18 .. Rec_loss: 31949165.18 .. NELBO: 34933430.59\n",
      "****************************************************************************************************\n",
      "Epoch: 82 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 197.23 .. KL_eta: 56.18 .. KL_alpha: 2973957.36 .. Rec_loss: 31836390.91 .. NELBO: 34810602.36\n",
      "****************************************************************************************************\n",
      "Epoch----->82 .. LR: 0.005 .. KL_theta: 174.42 .. KL_eta: 56.33 .. KL_alpha: 2961456.79 .. Rec_loss: 31778579.06 .. NELBO: 34740267.06\n",
      "****************************************************************************************************\n",
      "Epoch: 83 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 149.12 .. KL_eta: 54.44 .. KL_alpha: 2958229.27 .. Rec_loss: 31879520.0 .. NELBO: 34837953.09\n",
      "****************************************************************************************************\n",
      "Epoch----->83 .. LR: 0.005 .. KL_theta: 138.28 .. KL_eta: 54.32 .. KL_alpha: 2951236.65 .. Rec_loss: 31866969.76 .. NELBO: 34818398.94\n",
      "****************************************************************************************************\n",
      "Epoch: 84 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 101.76 .. KL_eta: 54.16 .. KL_alpha: 2947212.95 .. Rec_loss: 31583629.64 .. NELBO: 34530999.27\n",
      "****************************************************************************************************\n",
      "Epoch----->84 .. LR: 0.005 .. KL_theta: 96.18 .. KL_eta: 55.16 .. KL_alpha: 2939409.6 .. Rec_loss: 31823794.35 .. NELBO: 34763356.0\n",
      "****************************************************************************************************\n",
      "Epoch: 85 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 76.86 .. KL_eta: 56.73 .. KL_alpha: 2925558.77 .. Rec_loss: 31899015.27 .. NELBO: 34824707.64\n",
      "****************************************************************************************************\n",
      "Epoch----->85 .. LR: 0.005 .. KL_theta: 69.71 .. KL_eta: 55.84 .. KL_alpha: 2925256.19 .. Rec_loss: 31842970.59 .. NELBO: 34768352.24\n",
      "****************************************************************************************************\n",
      "Epoch: 86 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 90.81 .. KL_eta: 56.71 .. KL_alpha: 2921305.14 .. Rec_loss: 31686827.45 .. NELBO: 34608278.91\n",
      "****************************************************************************************************\n",
      "Epoch----->86 .. LR: 0.005 .. KL_theta: 156.75 .. KL_eta: 58.46 .. KL_alpha: 2917798.38 .. Rec_loss: 31916825.06 .. NELBO: 34834838.47\n",
      "****************************************************************************************************\n",
      "Epoch: 87 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 136.71 .. KL_eta: 65.38 .. KL_alpha: 2920337.5 .. Rec_loss: 31884886.36 .. NELBO: 34805426.0\n",
      "****************************************************************************************************\n",
      "Epoch----->87 .. LR: 0.005 .. KL_theta: 114.63 .. KL_eta: 62.61 .. KL_alpha: 2916331.18 .. Rec_loss: 31731252.12 .. NELBO: 34647760.71\n",
      "****************************************************************************************************\n",
      "Epoch: 88 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 61.21 .. KL_eta: 59.09 .. KL_alpha: 2898157.73 .. Rec_loss: 32003674.18 .. NELBO: 34901952.91\n",
      "****************************************************************************************************\n",
      "Epoch----->88 .. LR: 0.005 .. KL_theta: 55.19 .. KL_eta: 58.39 .. KL_alpha: 2904469.53 .. Rec_loss: 31872352.12 .. NELBO: 34776935.53\n",
      "****************************************************************************************************\n",
      "Epoch: 89 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 35.26 .. KL_eta: 54.72 .. KL_alpha: 2883451.39 .. Rec_loss: 31920933.64 .. NELBO: 34804474.55\n",
      "****************************************************************************************************\n",
      "Epoch----->89 .. LR: 0.005 .. KL_theta: 40.15 .. KL_eta: 55.83 .. KL_alpha: 2885822.29 .. Rec_loss: 31818384.59 .. NELBO: 34704302.82\n",
      "****************************************************************************************************\n",
      "Epoch: 90 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 86.96 .. KL_eta: 55.83 .. KL_alpha: 2869266.66 .. Rec_loss: 30978021.64 .. NELBO: 33847431.82\n",
      "****************************************************************************************************\n",
      "Epoch----->90 .. LR: 0.005 .. KL_theta: 90.8 .. KL_eta: 55.98 .. KL_alpha: 2871858.82 .. Rec_loss: 31726555.88 .. NELBO: 34598562.59\n",
      "****************************************************************************************************\n",
      "Epoch: 91 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 207.39 .. KL_eta: 57.62 .. KL_alpha: 2872716.07 .. Rec_loss: 32072284.0 .. NELBO: 34945264.18\n",
      "****************************************************************************************************\n",
      "Epoch----->91 .. LR: 0.005 .. KL_theta: 189.14 .. KL_eta: 59.88 .. KL_alpha: 2870649.18 .. Rec_loss: 31703853.06 .. NELBO: 34574750.71\n",
      "****************************************************************************************************\n",
      "Epoch: 92 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 76.27 .. KL_eta: 64.26 .. KL_alpha: 2838875.07 .. Rec_loss: 31449321.27 .. NELBO: 34288336.36\n",
      "****************************************************************************************************\n",
      "Epoch----->92 .. LR: 0.005 .. KL_theta: 67.53 .. KL_eta: 63.83 .. KL_alpha: 2841733.62 .. Rec_loss: 31940225.29 .. NELBO: 34782089.88\n",
      "****************************************************************************************************\n",
      "Epoch: 93 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 65.43 .. KL_eta: 61.94 .. KL_alpha: 2854950.84 .. Rec_loss: 31650319.82 .. NELBO: 34505398.18\n",
      "****************************************************************************************************\n",
      "Epoch----->93 .. LR: 0.005 .. KL_theta: 74.11 .. KL_eta: 61.36 .. KL_alpha: 2842174.06 .. Rec_loss: 31770568.82 .. NELBO: 34612878.82\n",
      "****************************************************************************************************\n",
      "Epoch: 94 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 73.97 .. KL_eta: 61.42 .. KL_alpha: 2815613.8 .. Rec_loss: 32052666.73 .. NELBO: 34868415.82\n",
      "****************************************************************************************************\n",
      "Epoch----->94 .. LR: 0.005 .. KL_theta: 75.67 .. KL_eta: 61.84 .. KL_alpha: 2821009.29 .. Rec_loss: 31880918.59 .. NELBO: 34702065.76\n",
      "****************************************************************************************************\n",
      "Epoch: 95 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 280.43 .. KL_eta: 72.76 .. KL_alpha: 2837900.23 .. Rec_loss: 31832171.82 .. NELBO: 34670426.18\n",
      "****************************************************************************************************\n",
      "Epoch----->95 .. LR: 0.005 .. KL_theta: 242.63 .. KL_eta: 74.48 .. KL_alpha: 2829269.35 .. Rec_loss: 31769363.29 .. NELBO: 34598950.12\n",
      "****************************************************************************************************\n",
      "Epoch: 96 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 173.21 .. KL_eta: 68.01 .. KL_alpha: 2809112.95 .. Rec_loss: 31980272.55 .. NELBO: 34789626.18\n",
      "****************************************************************************************************\n",
      "Epoch----->96 .. LR: 0.005 .. KL_theta: 145.06 .. KL_eta: 65.22 .. KL_alpha: 2810095.16 .. Rec_loss: 31811028.47 .. NELBO: 34621333.76\n",
      "****************************************************************************************************\n",
      "Epoch: 97 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 82.01 .. KL_eta: 56.45 .. KL_alpha: 2785849.39 .. Rec_loss: 32483292.18 .. NELBO: 35269281.09\n",
      "****************************************************************************************************\n",
      "Epoch----->97 .. LR: 0.005 .. KL_theta: 79.01 .. KL_eta: 55.99 .. KL_alpha: 2782641.04 .. Rec_loss: 31716473.76 .. NELBO: 34499250.24\n",
      "****************************************************************************************************\n",
      "Epoch: 98 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 80.13 .. KL_eta: 55.44 .. KL_alpha: 2789141.75 .. Rec_loss: 31947355.45 .. NELBO: 34736633.45\n",
      "****************************************************************************************************\n",
      "Epoch----->98 .. LR: 0.005 .. KL_theta: 99.29 .. KL_eta: 55.07 .. KL_alpha: 2787171.32 .. Rec_loss: 31742805.06 .. NELBO: 34530131.53\n",
      "****************************************************************************************************\n",
      "Epoch: 99 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 101.39 .. KL_eta: 54.05 .. KL_alpha: 2784920.43 .. Rec_loss: 31700207.45 .. NELBO: 34485283.82\n",
      "****************************************************************************************************\n",
      "Epoch----->99 .. LR: 0.005 .. KL_theta: 87.39 .. KL_eta: 54.64 .. KL_alpha: 2777411.13 .. Rec_loss: 31865574.12 .. NELBO: 34643127.65\n",
      "****************************************************************************************************\n",
      "Epoch: 100 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 47.73 .. KL_eta: 53.22 .. KL_alpha: 2757198.0 .. Rec_loss: 32106892.36 .. NELBO: 34864191.27\n",
      "****************************************************************************************************\n",
      "Epoch----->100 .. LR: 0.005 .. KL_theta: 59.22 .. KL_eta: 53.61 .. KL_alpha: 2759658.87 .. Rec_loss: 31892815.29 .. NELBO: 34652587.29\n",
      "****************************************************************************************************\n",
      "Epoch: 101 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 60.8 .. KL_eta: 56.65 .. KL_alpha: 2748423.57 .. Rec_loss: 31896236.55 .. NELBO: 34644778.73\n",
      "****************************************************************************************************\n",
      "Epoch----->101 .. LR: 0.005 .. KL_theta: 91.09 .. KL_eta: 54.75 .. KL_alpha: 2747174.32 .. Rec_loss: 31776246.94 .. NELBO: 34523567.65\n",
      "****************************************************************************************************\n",
      "Epoch: 102 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 294.09 .. KL_eta: 55.27 .. KL_alpha: 2733257.84 .. Rec_loss: 31430939.64 .. NELBO: 34164546.91\n",
      "****************************************************************************************************\n",
      "Epoch----->102 .. LR: 0.005 .. KL_theta: 281.97 .. KL_eta: 58.9 .. KL_alpha: 2732718.4 .. Rec_loss: 31916164.47 .. NELBO: 34649223.53\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Epoch----->107 .. LR: 0.005 .. KL_theta: 49.83 .. KL_eta: 56.54 .. KL_alpha: 2685088.74 .. Rec_loss: 31795704.59 .. NELBO: 34480899.29\n",
      "****************************************************************************************************\n",
      "Epoch: 108 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 37.39 .. KL_eta: 57.14 .. KL_alpha: 2674610.64 .. Rec_loss: 31948517.45 .. NELBO: 34623222.91\n",
      "****************************************************************************************************\n",
      "Epoch----->108 .. LR: 0.005 .. KL_theta: 34.63 .. KL_eta: 57.02 .. KL_alpha: 2677282.57 .. Rec_loss: 31812025.76 .. NELBO: 34489400.35\n",
      "****************************************************************************************************\n",
      "Epoch: 109 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 35.39 .. KL_eta: 56.33 .. KL_alpha: 2670766.55 .. Rec_loss: 31788576.36 .. NELBO: 34459434.36\n",
      "****************************************************************************************************\n",
      "Epoch----->109 .. LR: 0.005 .. KL_theta: 121.86 .. KL_eta: 58.41 .. KL_alpha: 2663063.65 .. Rec_loss: 31776655.06 .. NELBO: 34439898.59\n",
      "****************************************************************************************************\n",
      "Epoch: 110 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 114.87 .. KL_eta: 60.14 .. KL_alpha: 2648049.45 .. Rec_loss: 32179257.27 .. NELBO: 34827482.36\n",
      "****************************************************************************************************\n",
      "Epoch----->110 .. LR: 0.005 .. KL_theta: 101.7 .. KL_eta: 59.79 .. KL_alpha: 2645406.5 .. Rec_loss: 31800682.47 .. NELBO: 34446251.18\n",
      "****************************************************************************************************\n",
      "Epoch: 111 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 52.48 .. KL_eta: 54.91 .. KL_alpha: 2656894.93 .. Rec_loss: 31768062.73 .. NELBO: 34425065.64\n",
      "****************************************************************************************************\n",
      "Epoch----->111 .. LR: 0.005 .. KL_theta: 78.11 .. KL_eta: 54.73 .. KL_alpha: 2647602.32 .. Rec_loss: 31956075.88 .. NELBO: 34603811.29\n",
      "****************************************************************************************************\n",
      "Epoch: 112 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 865.94 .. KL_eta: 68.6 .. KL_alpha: 2635414.52 .. Rec_loss: 31898379.27 .. NELBO: 34534729.45\n",
      "****************************************************************************************************\n",
      "Epoch----->112 .. LR: 0.005 .. KL_theta: 732.65 .. KL_eta: 75.78 .. KL_alpha: 2636793.78 .. Rec_loss: 31859615.53 .. NELBO: 34497218.71\n",
      "****************************************************************************************************\n",
      "Epoch: 113 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 312.7 .. KL_eta: 74.57 .. KL_alpha: 2648806.27 .. Rec_loss: 31871821.64 .. NELBO: 34521015.45\n",
      "****************************************************************************************************\n",
      "Epoch----->113 .. LR: 0.005 .. KL_theta: 282.21 .. KL_eta: 80.56 .. KL_alpha: 2640312.59 .. Rec_loss: 31896077.76 .. NELBO: 34536753.29\n",
      "****************************************************************************************************\n",
      "Epoch: 114 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 150.56 .. KL_eta: 79.01 .. KL_alpha: 2613623.3 .. Rec_loss: 32293801.82 .. NELBO: 34907653.64\n",
      "****************************************************************************************************\n",
      "Epoch----->114 .. LR: 0.005 .. KL_theta: 139.5 .. KL_eta: 71.86 .. KL_alpha: 2616144.6 .. Rec_loss: 31794352.35 .. NELBO: 34410707.65\n",
      "****************************************************************************************************\n",
      "Epoch: 115 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 50.78 .. KL_eta: 57.99 .. KL_alpha: 2611341.36 .. Rec_loss: 31764938.73 .. NELBO: 34376388.18\n",
      "****************************************************************************************************\n",
      "Epoch----->115 .. LR: 0.005 .. KL_theta: 53.68 .. KL_eta: 57.3 .. KL_alpha: 2611759.66 .. Rec_loss: 31897269.18 .. NELBO: 34509139.06\n",
      "****************************************************************************************************\n",
      "Epoch: 116 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 78.82 .. KL_eta: 58.34 .. KL_alpha: 2609295.34 .. Rec_loss: 31913789.27 .. NELBO: 34523221.64\n",
      "****************************************************************************************************\n",
      "Epoch----->116 .. LR: 0.005 .. KL_theta: 70.08 .. KL_eta: 58.77 .. KL_alpha: 2609765.63 .. Rec_loss: 31840181.76 .. NELBO: 34450076.24\n",
      "****************************************************************************************************\n",
      "Epoch: 117 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 31.93 .. KL_eta: 56.53 .. KL_alpha: 2592551.66 .. Rec_loss: 31990866.0 .. NELBO: 34583505.45\n",
      "****************************************************************************************************\n",
      "Epoch----->117 .. LR: 0.005 .. KL_theta: 29.8 .. KL_eta: 56.99 .. KL_alpha: 2586360.03 .. Rec_loss: 31901483.76 .. NELBO: 34487929.88\n",
      "****************************************************************************************************\n",
      "Epoch: 118 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 89.64 .. KL_eta: 58.2 .. KL_alpha: 2591786.57 .. Rec_loss: 32436715.09 .. NELBO: 35028649.27\n",
      "****************************************************************************************************\n",
      "Epoch----->118 .. LR: 0.005 .. KL_theta: 105.58 .. KL_eta: 59.09 .. KL_alpha: 2586840.51 .. Rec_loss: 31941277.76 .. NELBO: 34528282.94\n",
      "****************************************************************************************************\n",
      "Epoch: 119 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 93.0 .. KL_eta: 56.86 .. KL_alpha: 2586046.93 .. Rec_loss: 32108092.18 .. NELBO: 34694288.73\n",
      "****************************************************************************************************\n",
      "Epoch----->119 .. LR: 0.005 .. KL_theta: 78.82 .. KL_eta: 56.2 .. KL_alpha: 2581474.91 .. Rec_loss: 31853382.24 .. NELBO: 34434991.53\n",
      "****************************************************************************************************\n",
      "Epoch: 120 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 42.11 .. KL_eta: 56.93 .. KL_alpha: 2558022.45 .. Rec_loss: 32021458.0 .. NELBO: 34579578.91\n",
      "****************************************************************************************************\n",
      "Epoch----->120 .. LR: 0.005 .. KL_theta: 38.2 .. KL_eta: 56.96 .. KL_alpha: 2559754.46 .. Rec_loss: 31977031.88 .. NELBO: 34536881.06\n",
      "****************************************************************************************************\n",
      "Epoch: 121 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 31.38 .. KL_eta: 55.02 .. KL_alpha: 2542675.27 .. Rec_loss: 32235203.45 .. NELBO: 34777964.36\n",
      "****************************************************************************************************\n",
      "Epoch----->121 .. LR: 0.005 .. KL_theta: 28.01 .. KL_eta: 54.96 .. KL_alpha: 2547392.76 .. Rec_loss: 31809163.41 .. NELBO: 34356638.71\n",
      "****************************************************************************************************\n",
      "Epoch: 122 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 20.21 .. KL_eta: 55.5 .. KL_alpha: 2534637.61 .. Rec_loss: 32197027.64 .. NELBO: 34731741.27\n",
      "****************************************************************************************************\n",
      "Epoch----->122 .. LR: 0.005 .. KL_theta: 20.05 .. KL_eta: 55.26 .. KL_alpha: 2538611.71 .. Rec_loss: 31864551.06 .. NELBO: 34403238.47\n",
      "****************************************************************************************************\n",
      "Epoch: 123 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 21.63 .. KL_eta: 55.34 .. KL_alpha: 2536292.66 .. Rec_loss: 31846588.91 .. NELBO: 34382959.27\n",
      "****************************************************************************************************\n",
      "Epoch----->123 .. LR: 0.005 .. KL_theta: 23.4 .. KL_eta: 55.03 .. KL_alpha: 2534710.01 .. Rec_loss: 31990050.82 .. NELBO: 34524840.24\n",
      "****************************************************************************************************\n",
      "Epoch: 124 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 23.51 .. KL_eta: 54.05 .. KL_alpha: 2528163.2 .. Rec_loss: 31992098.73 .. NELBO: 34520340.0\n",
      "****************************************************************************************************\n",
      "Epoch----->124 .. LR: 0.005 .. KL_theta: 21.56 .. KL_eta: 53.55 .. KL_alpha: 2519669.91 .. Rec_loss: 31819038.94 .. NELBO: 34338784.12\n",
      "****************************************************************************************************\n",
      "Epoch: 125 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 151.97 .. KL_eta: 55.52 .. KL_alpha: 2526087.59 .. Rec_loss: 31952201.09 .. NELBO: 34478496.91\n",
      "****************************************************************************************************\n",
      "Epoch----->125 .. LR: 0.005 .. KL_theta: 162.23 .. KL_eta: 55.67 .. KL_alpha: 2515271.63 .. Rec_loss: 31933762.12 .. NELBO: 34449251.88\n",
      "****************************************************************************************************\n",
      "Epoch: 126 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 66.33 .. KL_eta: 54.48 .. KL_alpha: 2504661.0 .. Rec_loss: 31980020.36 .. NELBO: 34484802.18\n",
      "****************************************************************************************************\n",
      "Epoch----->126 .. LR: 0.005 .. KL_theta: 77.9 .. KL_eta: 53.99 .. KL_alpha: 2502912.72 .. Rec_loss: 31779504.12 .. NELBO: 34282549.06\n",
      "****************************************************************************************************\n",
      "Epoch: 127 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 93.25 .. KL_eta: 56.84 .. KL_alpha: 2510212.95 .. Rec_loss: 31794602.0 .. NELBO: 34304965.27\n",
      "****************************************************************************************************\n",
      "Epoch----->127 .. LR: 0.005 .. KL_theta: 82.87 .. KL_eta: 56.18 .. KL_alpha: 2503459.99 .. Rec_loss: 31961258.35 .. NELBO: 34464857.41\n",
      "****************************************************************************************************\n",
      "Epoch: 128 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 44.34 .. KL_eta: 52.91 .. KL_alpha: 2487452.45 .. Rec_loss: 32277180.0 .. NELBO: 34764730.36\n",
      "****************************************************************************************************\n",
      "Epoch----->128 .. LR: 0.005 .. KL_theta: 41.39 .. KL_eta: 53.54 .. KL_alpha: 2486541.54 .. Rec_loss: 31897823.06 .. NELBO: 34384459.88\n",
      "****************************************************************************************************\n",
      "Epoch: 129 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 27.36 .. KL_eta: 55.17 .. KL_alpha: 2479733.18 .. Rec_loss: 31678856.18 .. NELBO: 34158671.64\n",
      "****************************************************************************************************\n",
      "Epoch----->129 .. LR: 0.005 .. KL_theta: 25.48 .. KL_eta: 54.81 .. KL_alpha: 2481039.31 .. Rec_loss: 31892402.0 .. NELBO: 34373521.65\n",
      "****************************************************************************************************\n",
      "Epoch: 130 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 2423.48 .. KL_eta: 93.13 .. KL_alpha: 2462108.36 .. Rec_loss: 31740638.91 .. NELBO: 34205263.82\n",
      "****************************************************************************************************\n",
      "Epoch----->130 .. LR: 0.005 .. KL_theta: 1878.33 .. KL_eta: 114.93 .. KL_alpha: 2463798.41 .. Rec_loss: 31907708.59 .. NELBO: 34373500.35\n",
      "****************************************************************************************************\n",
      "Epoch: 131 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 245.43 .. KL_eta: 99.95 .. KL_alpha: 2453989.34 .. Rec_loss: 31884347.64 .. NELBO: 34338682.36\n",
      "****************************************************************************************************\n",
      "Epoch----->131 .. LR: 0.005 .. KL_theta: 194.54 .. KL_eta: 106.1 .. KL_alpha: 2458016.16 .. Rec_loss: 31714012.71 .. NELBO: 34172329.53\n",
      "****************************************************************************************************\n",
      "Epoch: 132 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 264.29 .. KL_eta: 147.04 .. KL_alpha: 2467493.7 .. Rec_loss: 31890577.82 .. NELBO: 34358482.55\n",
      "****************************************************************************************************\n",
      "Epoch----->132 .. LR: 0.005 .. KL_theta: 224.65 .. KL_eta: 142.58 .. KL_alpha: 2462297.12 .. Rec_loss: 31924544.47 .. NELBO: 34387208.71\n",
      "****************************************************************************************************\n",
      "Epoch: 133 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 53.33 .. KL_eta: 77.65 .. KL_alpha: 2436281.34 .. Rec_loss: 32037318.18 .. NELBO: 34473730.91\n",
      "****************************************************************************************************\n",
      "Epoch----->133 .. LR: 0.005 .. KL_theta: 44.56 .. KL_eta: 75.01 .. KL_alpha: 2442099.1 .. Rec_loss: 31862218.35 .. NELBO: 34304437.06\n",
      "****************************************************************************************************\n",
      "Epoch: 134 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 47.38 .. KL_eta: 67.3 .. KL_alpha: 2445803.68 .. Rec_loss: 31633030.18 .. NELBO: 34078948.73\n",
      "****************************************************************************************************\n",
      "Epoch----->134 .. LR: 0.005 .. KL_theta: 64.9 .. KL_eta: 66.78 .. KL_alpha: 2443225.25 .. Rec_loss: 31890003.41 .. NELBO: 34333360.47\n",
      "****************************************************************************************************\n",
      "Epoch: 135 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 45.61 .. KL_eta: 61.84 .. KL_alpha: 2432336.39 .. Rec_loss: 31245973.64 .. NELBO: 33678416.91\n",
      "****************************************************************************************************\n",
      "Epoch----->135 .. LR: 0.005 .. KL_theta: 50.86 .. KL_eta: 60.6 .. KL_alpha: 2425003.21 .. Rec_loss: 31956800.47 .. NELBO: 34381914.71\n",
      "****************************************************************************************************\n",
      "Epoch: 136 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 31.28 .. KL_eta: 57.98 .. KL_alpha: 2427444.45 .. Rec_loss: 32224862.73 .. NELBO: 34652396.73\n",
      "****************************************************************************************************\n",
      "Epoch----->136 .. LR: 0.005 .. KL_theta: 98.55 .. KL_eta: 58.3 .. KL_alpha: 2421785.53 .. Rec_loss: 31816150.59 .. NELBO: 34238093.41\n",
      "****************************************************************************************************\n",
      "Epoch: 137 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 153.05 .. KL_eta: 61.13 .. KL_alpha: 2416591.41 .. Rec_loss: 31730361.09 .. NELBO: 34147166.36\n",
      "****************************************************************************************************\n",
      "Epoch----->137 .. LR: 0.005 .. KL_theta: 121.67 .. KL_eta: 60.36 .. KL_alpha: 2407522.99 .. Rec_loss: 31817722.47 .. NELBO: 34225427.88\n",
      "****************************************************************************************************\n",
      "Epoch: 138 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 171.99 .. KL_eta: 63.66 .. KL_alpha: 2412015.41 .. Rec_loss: 32148618.0 .. NELBO: 34560869.45\n",
      "****************************************************************************************************\n",
      "Epoch----->138 .. LR: 0.005 .. KL_theta: 150.0 .. KL_eta: 64.2 .. KL_alpha: 2409135.24 .. Rec_loss: 31737309.76 .. NELBO: 34146659.76\n",
      "****************************************************************************************************\n",
      "Epoch: 139 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 55.03 .. KL_eta: 57.7 .. KL_alpha: 2390638.86 .. Rec_loss: 31950090.91 .. NELBO: 34340842.36\n",
      "****************************************************************************************************\n",
      "Epoch----->139 .. LR: 0.005 .. KL_theta: 47.03 .. KL_eta: 56.2 .. KL_alpha: 2393546.43 .. Rec_loss: 31929534.47 .. NELBO: 34323184.24\n",
      "****************************************************************************************************\n",
      "Epoch: 140 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 40.86 .. KL_eta: 55.98 .. KL_alpha: 2378653.07 .. Rec_loss: 31933490.18 .. NELBO: 34312240.18\n",
      "****************************************************************************************************\n",
      "Epoch----->140 .. LR: 0.005 .. KL_theta: 45.29 .. KL_eta: 55.2 .. KL_alpha: 2386253.18 .. Rec_loss: 31936142.12 .. NELBO: 34322496.0\n",
      "****************************************************************************************************\n",
      "Epoch: 141 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 38.32 .. KL_eta: 55.3 .. KL_alpha: 2368245.77 .. Rec_loss: 31583972.18 .. NELBO: 33952310.55\n",
      "****************************************************************************************************\n",
      "Epoch----->141 .. LR: 0.005 .. KL_theta: 31.19 .. KL_eta: 54.15 .. KL_alpha: 2372057.69 .. Rec_loss: 31842502.12 .. NELBO: 34214644.59\n",
      "****************************************************************************************************\n",
      "Epoch: 142 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 17.69 .. KL_eta: 53.33 .. KL_alpha: 2370831.39 .. Rec_loss: 31819573.82 .. NELBO: 34190476.91\n",
      "****************************************************************************************************\n",
      "Epoch----->142 .. LR: 0.005 .. KL_theta: 17.64 .. KL_eta: 53.5 .. KL_alpha: 2376050.16 .. Rec_loss: 31879428.59 .. NELBO: 34255550.12\n",
      "****************************************************************************************************\n",
      "Epoch: 143 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 16.08 .. KL_eta: 52.41 .. KL_alpha: 2355011.75 .. Rec_loss: 31639658.36 .. NELBO: 33994738.55\n",
      "****************************************************************************************************\n",
      "Epoch----->143 .. LR: 0.005 .. KL_theta: 16.89 .. KL_eta: 52.82 .. KL_alpha: 2360818.25 .. Rec_loss: 31867597.76 .. NELBO: 34228486.35\n",
      "****************************************************************************************************\n",
      "Epoch: 144 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 16.52 .. KL_eta: 52.44 .. KL_alpha: 2362061.52 .. Rec_loss: 32123674.55 .. NELBO: 34485804.91\n",
      "****************************************************************************************************\n",
      "Epoch----->144 .. LR: 0.005 .. KL_theta: 15.57 .. KL_eta: 51.65 .. KL_alpha: 2356918.25 .. Rec_loss: 31834703.76 .. NELBO: 34191689.18\n",
      "****************************************************************************************************\n",
      "Epoch: 145 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 19.21 .. KL_eta: 51.42 .. KL_alpha: 2363159.98 .. Rec_loss: 31913920.55 .. NELBO: 34277152.18\n",
      "****************************************************************************************************\n",
      "Epoch----->145 .. LR: 0.005 .. KL_theta: 23.37 .. KL_eta: 51.9 .. KL_alpha: 2363052.56 .. Rec_loss: 31722715.41 .. NELBO: 34085843.53\n",
      "****************************************************************************************************\n",
      "Epoch: 146 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 45.43 .. KL_eta: 52.95 .. KL_alpha: 2349207.8 .. Rec_loss: 31956524.18 .. NELBO: 34305830.36\n",
      "****************************************************************************************************\n",
      "Epoch----->146 .. LR: 0.005 .. KL_theta: 42.69 .. KL_eta: 53.3 .. KL_alpha: 2344332.21 .. Rec_loss: 31794229.18 .. NELBO: 34138657.06\n",
      "****************************************************************************************************\n",
      "Epoch: 147 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 26.35 .. KL_eta: 54.9 .. KL_alpha: 2341108.61 .. Rec_loss: 31805490.18 .. NELBO: 34146679.82\n",
      "****************************************************************************************************\n",
      "Epoch----->147 .. LR: 0.005 .. KL_theta: 22.77 .. KL_eta: 53.71 .. KL_alpha: 2337699.68 .. Rec_loss: 31943324.59 .. NELBO: 34281100.59\n",
      "****************************************************************************************************\n",
      "Epoch: 148 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 89.0 .. KL_eta: 54.13 .. KL_alpha: 2328992.45 .. Rec_loss: 30867702.18 .. NELBO: 33196837.27\n",
      "****************************************************************************************************\n",
      "Epoch----->148 .. LR: 0.005 .. KL_theta: 170.59 .. KL_eta: 56.32 .. KL_alpha: 2324750.38 .. Rec_loss: 32016082.47 .. NELBO: 34341059.18\n",
      "****************************************************************************************************\n",
      "Epoch: 149 .. batch: 10/17 .. LR: 0.005 .. KL_theta: 205.27 .. KL_eta: 68.93 .. KL_alpha: 2302447.34 .. Rec_loss: 31989702.73 .. NELBO: 34292424.36\n",
      "****************************************************************************************************\n",
      "Epoch----->149 .. LR: 0.005 .. KL_theta: 152.52 .. KL_eta: 65.87 .. KL_alpha: 2309828.37 .. Rec_loss: 31867462.47 .. NELBO: 34177509.65\n",
      "****************************************************************************************************\n",
      "saving topic matrix beta...\n",
      "CPU times: user 1d 15h 44min 3s, sys: 39min 50s, total: 1d 16h 23min 54s\n",
      "Wall time: 51min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## train model on data by looping through multiple epochs\n",
    "best_epoch = 0\n",
    "best_val_ppl = 1e9\n",
    "all_val_ppls = []\n",
    "for epoch in range(1, args.epochs):\n",
    "    train(epoch)\n",
    "    ## check whether to anneal lr\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    if args.anneal_lr and (len(all_val_ppls) > args.nonmono and val_ppl > min(all_val_ppls[:-args.nonmono]) and lr > 1e-5):\n",
    "        optimizer.param_groups[0]['lr'] /= args.lr_factor\n",
    "    #all_val_ppls.append(val_ppl)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('saving topic matrix beta...')\n",
    "    alpha = model.mu_q_alpha\n",
    "    beta = model.get_beta(alpha).cpu().numpy()\n",
    "    scipy.io.savemat(ckpt+'_beta.mat', {'values': beta}, do_compression=True)\n",
    "    if args.train_embeddings:\n",
    "        print('saving word embedding matrix rho...')\n",
    "        rho = model.rho.weight.cpu().numpy()\n",
    "        scipy.io.savemat(ckpt+'_rho.mat', {'values': rho}, do_compression=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60ce5b-33c9-46d5-8052-4011e8ff7559",
   "metadata": {},
   "source": [
    "## see if i can get a measure of topic coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e47996a-e7d0-4d97-945c-6929968fd00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/combined_clean.parquet')\n",
    "id2word = Dictionary([vocab])\n",
    "split_text = df['filtered_text'].str.split().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1e2940c-e1ee-4610-87cf-c02b84767760",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff7e94ff-629a-49ca-b780-df360a861b91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta:  torch.Size([10, 4, 4938])\n",
      "\n",
      "\n",
      "####################################################################################################\n",
      "Visualize topics...\n",
      "Topic 0 .. Time: 0 ===> ['certificate', 'surface', 'supplementary', 'supplemental', 'burn', 'cable', 'calculate', 'suit', 'calendar']\n",
      "Topic 0 .. Time: 2 ===> ['random', 'olivier', 'vicious', 'soviet', 'felix', 'comfortably', 'slump', 'similarity', 'administrative']\n",
      "Topic 1 .. Time: 0 ===> ['certificate', 'strained', 'universe', 'newspaper', 'cascade', 'night', 'impression', 'casualty', 'cautious']\n",
      "Topic 1 .. Time: 2 ===> ['random', 'cpi', 'circle', 'olivier', 'closure', 'cluster', 'vicious', 'soviet', 'felix']\n",
      "Topic 2 .. Time: 0 ===> ['random', 'cpi', 'circle', 'olivier', 'closure', 'cluster', 'vicious', 'soviet', 'felix']\n",
      "Topic 2 .. Time: 2 ===> ['enrollment', 'master', 'translate', 'dominance', 'temptation', 'exotic', 'highway', 'frank', 'cpi']\n",
      "Topic 3 .. Time: 0 ===> ['random', 'olivier', 'vicious', 'soviet', 'felix', 'comfortably', 'slump', 'similarity', 'administrative']\n",
      "Topic 3 .. Time: 2 ===> ['random', 'circle', 'closure', 'soviet', 'felix', 'comfortably', 'administrative', 'slump', 'similarity']\n",
      "Topic 4 .. Time: 0 ===> ['certificate', 'analytic', 'unreasonable', 'friendly', 'analogy', 'analogous', 'substitution', 'amplitude', 'inability']\n",
      "Topic 4 .. Time: 2 ===> ['random', 'circle', 'closure', 'soviet', 'felix', 'comfortably', 'administrative', 'slump', 'similarity']\n",
      "Topic 5 .. Time: 0 ===> ['certificate', 'surface', 'supplementary', 'supplemental', 'burn', 'cable', 'calculate', 'suit', 'calendar']\n",
      "Topic 5 .. Time: 2 ===> ['certificate', 'surface', 'supplementary', 'supplemental', 'burn', 'cable', 'calculate', 'suit', 'calendar']\n",
      "Topic 6 .. Time: 0 ===> ['certificate', 'surface', 'supplementary', 'supplemental', 'burn', 'cable', 'calculate', 'suit', 'calendar']\n",
      "Topic 6 .. Time: 2 ===> ['certificate', 'surface', 'supplementary', 'supplemental', 'burn', 'cable', 'calculate', 'suit', 'calendar']\n",
      "Topic 7 .. Time: 0 ===> ['random', 'olivier', 'vicious', 'soviet', 'felix', 'comfortably', 'slump', 'similarity', 'administrative']\n",
      "Topic 7 .. Time: 2 ===> ['certificate', 'surface', 'supplementary', 'supplemental', 'burn', 'cable', 'calculate', 'suit', 'calendar']\n",
      "Topic 8 .. Time: 0 ===> ['random', 'olivier', 'vicious', 'soviet', 'felix', 'comfortably', 'slump', 'similarity', 'administrative']\n",
      "Topic 8 .. Time: 2 ===> ['random', 'cpi', 'circle', 'olivier', 'closure', 'cluster', 'vicious', 'soviet', 'felix']\n",
      "Topic 9 .. Time: 0 ===> ['random', 'vicious', 'centre', 'progressively', 'utmost', 'absorption', 'foreigner', 'chronic', 'churning']\n",
      "Topic 9 .. Time: 2 ===> ['certificate', 'surface', 'supplementary', 'supplemental', 'burn', 'cable', 'calculate', 'suit', 'calendar']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    alpha = model.mu_q_alpha\n",
    "    beta = model.get_beta(alpha) \n",
    "    print('beta: ', beta.size())\n",
    "    print('\\n')\n",
    "    print('#'*100)\n",
    "    print('Visualize topics...')\n",
    "    times = [0, 2]\n",
    "    topics_words = []\n",
    "    for k in range(args.num_topics):\n",
    "        for t in times:\n",
    "            gamma = beta[k, t, :]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-args.num_words+1:][::-1])\n",
    "            topic_words = [id2word[a] for a in top_words]\n",
    "            topics_words.append(' '.join(topic_words))\n",
    "            print('Topic {} .. Time: {} ===> {}'.format(k, t, topic_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55e5a6f0-9fec-4550-9b17-0e536c9595bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:30<00:00,  7.70s/it]\n"
     ]
    }
   ],
   "source": [
    "coherences = []\n",
    "for t in tqdm(range(args.num_times)):\n",
    "    coherences.append(\n",
    "        CoherenceModel(\n",
    "            topics=get_detm_topics(beta=beta, time=t, num_words=20, vocab=id2word, num_topics=args.num_topics), # use 20 words to standardize with DTM\n",
    "            texts=split_text, \n",
    "            dictionary=id2word, \n",
    "            coherence='c_v'\n",
    "        ).get_coherence()\n",
    "    )\n",
    "\n",
    "coherences = np.array(coherences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46a4b7b1-7a78-4d78-b42c-6845c032bb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:00<00:00, 1043.81it/s]\n"
     ]
    }
   ],
   "source": [
    "diversities = []\n",
    "for t in tqdm(range(args.num_times)):\n",
    "    diversities.append(\n",
    "        topic_diversity(topics=get_detm_topics(beta=beta, time=t, num_words=20, vocab=id2word, num_topics=args.num_topics))\n",
    "    )\n",
    "    \n",
    "diversities = np.array(diversities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1a4184d-93eb-4844-87c7-174d5ca8f485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24439425577015977, 0.023484462422265148)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qualities = diversities * coherences\n",
    "qualities.mean(), qualities.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6294257d-5fc9-45d2-aab7-c57b24a99af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.67120938, 0.66145312, 0.68506138, 0.66313999]),\n",
       " array([0.395, 0.35 , 0.31 , 0.405]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherences, diversities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01cf7948-c9df-454d-82e1-8c1b95b787ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\n",
    "    'detm_stats_bert_context.npz',\n",
    "    coherence=coherences,\n",
    "    diversity=diversities\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb012b8b-271c-456c-bbd7-7132a5861bdf",
   "metadata": {},
   "source": [
    "## try to dig into topic evolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cbb7f4a-17a7-4e93-b15a-c708218907ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = scipy.io.loadmat(ckpt+'_beta.mat')['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "438c420a-b45e-40c9-98df-1a65bdfa06a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_probs(word, topic, beta):\n",
    "    word_id = id2word.token2id[word]\n",
    "    probs = []\n",
    "    for t in range(4):\n",
    "        gamma = beta[topic, t, :]\n",
    "        probs.append(gamma[word_id])\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c01b3-176c-4a70-b99e-1aebe834a10a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
